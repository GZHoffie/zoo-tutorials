{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "centered-madonna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-disaster",
   "metadata": {},
   "source": [
    "To start using zoo.orca, we need to first initialize orca context. Here we specify local or distributed mode. In this example, we choose the local mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "foster-physiology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing orca context\n",
      "Current pyspark location is : /intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\n",
      "Start to getOrCreate SparkContext\n",
      "pyspark_submit_args is:  --driver-class-path /home/zhenhao/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-dist-all/lib/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/zhenhao/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-dist-all/lib/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/intern/spark/spark-2.4.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:02:10 WARN  Utils:66 - Your hostname, intern01 resolves to a loopback address: 127.0.1.1; using 10.239.44.107 instead (on interface eno1)\n",
      "2021-02-04 17:02:10 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2021-02-04 17:02:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-04 17:02:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.Sample\n",
      "BigDLBasePickler registering: bigdl.util.common  Sample\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.EvaluatedResult\n",
      "BigDLBasePickler registering: bigdl.util.common  EvaluatedResult\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JTensor\n",
      "BigDLBasePickler registering: bigdl.util.common  JTensor\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JActivity\n",
      "BigDLBasePickler registering: bigdl.util.common  JActivity\n",
      "Successfully got a SparkContext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_INIT_WAIT=2048\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NEXT_WAIT=1024\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=4M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=8\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='1'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=4M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from zoo.orca import init_orca_context, stop_orca_context\n",
    "from zoo.orca import OrcaContext\n",
    "\n",
    "OrcaContext.log_output = True # recommended to set it to True when running Analytics Zoo in Jupyter notebook (this will display terminal's  stdout and stderr in the Jupyter notebook).\n",
    "\n",
    "cluster_mode = \"local\"\n",
    "\n",
    "if cluster_mode == \"local\":  \n",
    "    init_orca_context(cluster_mode=\"local\", cores=4) # run in local mode\n",
    "elif cluster_mode == \"k8s\":  \n",
    "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=2) # run on K8s cluster\n",
    "elif cluster_mode == \"yarn\":  \n",
    "    init_orca_context(cluster_mode=\"yarn-client\", num_nodes=2, cores=2) # run on Hadoop YARN cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-northern",
   "metadata": {},
   "source": [
    "# Text generation with LSTM\n",
    "This notebook contains the code samples found in Chapter 8, Section 1 of Deep Learning with Python. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "[...]\n",
    "\n",
    "## Implementing character-level LSTM text generation\n",
    "Let's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the English language.\n",
    "\n",
    "## Preparing the data\n",
    "Let's start by downloading the corpus and converting it to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intelligent-advantage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-reach",
   "metadata": {},
   "source": [
    "Next, we will extract partially-overlapping sequences of length maxlen, one-hot encode them and pack them in a 3D Numpy array `x` of shape `(sequences, maxlen, unique_characters)`. Simultaneously, we prepare a array `y` containing the corresponding targets: the one-hot encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "anonymous-basin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique characters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "productive-tragedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200278, 60, 57)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "democratic-candle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200278, 57)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-commercial",
   "metadata": {},
   "source": [
    "\n",
    "## Building the network\n",
    "Our network is a single LSTM layer followed by a Dense classifier and softmax over all possible characters. But let us note that recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in recent times. Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "moving-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model(config):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "    model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-ending",
   "metadata": {},
   "source": [
    "We also create a `data_creator` function that help us import the data into the orca estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "familiar-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train_data_creator(config):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(config[\"batch_size\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-intake",
   "metadata": {},
   "source": [
    "Training the language model and sampling from it\n",
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "\n",
    "* 1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "* 2) Reweighting the distribution to a certain \"temperature\"\n",
    "* 3) Sampling the next character at random according to the reweighted distribution\n",
    "* 4) Adding the new character at the end of the available text\n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model, and draw a character index from it (the \"sampling function\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demanding-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-database",
   "metadata": {},
   "source": [
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "informed-thing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:02:48,985\tINFO resource_spec.py:212 -- Starting Ray with 17.29 GiB memory available for workers and up to 8.66 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_ip_address': '10.239.44.107', 'redis_address': '10.239.44.107:35472', 'object_store_address': '/tmp/ray/session_2021-02-04_17-02-48_984157_17398/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2021-02-04_17-02-48_984157_17398/sockets/raylet', 'webui_url': None, 'session_dir': '/tmp/ray/session_2021-02-04_17-02-48_984157_17398'}\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.042386: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.042423: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.917144: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.917166: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.917181: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (intern01): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.917366: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.944544: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3591785000 Hz\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.944735: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55675e7428f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.944754: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.948871: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.239.44.107:46809}\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m 2021-02-04 17:02:50.949122: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://10.239.44.107:46809\n"
     ]
    }
   ],
   "source": [
    "from zoo.orca.learn.tf2 import Estimator\n",
    "import random\n",
    "import sys\n",
    "\n",
    "batch_size=128\n",
    "est = Estimator.from_keras(model_creator=build_model,\n",
    "                           config={},\n",
    "                           workers_per_node=1,\n",
    "                           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "super-mount",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:From /home/zhenhao/.local/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "--- Generating with seed: \"heavy, difficult, dangerous thoughts, and\n",
      "a tempo of the gal\"\n",
      "------ temperature: 0.2\n",
      "heavy, difficult, dangerous thoughts, and\n",
      "a tempo of the gal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:14,653\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:14,666\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:14,811\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:14,813\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:14,905\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:14,905\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:14,906\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:14,908\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 2:============================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 185us/step\n",
      "l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:16,628\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:16,634\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:16,656\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:16,700\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:17,444\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:17,510\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:17,568\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:17,582\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 7:============================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 181us/step\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:18,408\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:18,432\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:18,870\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:18,956\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:19,201\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:19,244\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:19,706\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:19,743\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 12:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 170us/step\n",
      "t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:20,431\tWARNING worker.py:1072 -- WARNING: 12 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:20,488\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:20,729\tWARNING worker.py:1072 -- WARNING: 13 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:20,730\tWARNING worker.py:1072 -- WARNING: 13 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:20,854\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:20,924\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:20,966\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:21,632\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:21,731\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 17:==============>                                           (1 + 3) / 4]2021-02-04 17:05:21,821\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:21,852\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 17:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 178us/step\n",
      "h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:22,590\tWARNING worker.py:1072 -- WARNING: 14 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:22,591\tWARNING worker.py:1072 -- WARNING: 15 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:22,609\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:22,652\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:22,660\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:22,863\tWARNING worker.py:1072 -- WARNING: 16 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:22,863\tWARNING worker.py:1072 -- WARNING: 16 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:22,864\tWARNING worker.py:1072 -- WARNING: 16 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:22,867\tWARNING worker.py:1072 -- WARNING: 16 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:23,041\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:23,299\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:23,301\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:23,340\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:23,743\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 22:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 150us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78f0226560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:24,533\tWARNING worker.py:1072 -- WARNING: 17 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:24,544\tWARNING worker.py:1072 -- WARNING: 18 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:24,565\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:24,566\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:24,798\tWARNING worker.py:1072 -- WARNING: 19 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:24,805\tWARNING worker.py:1072 -- WARNING: 19 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:24,805\tWARNING worker.py:1072 -- WARNING: 19 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:24,983\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:25,152\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:25,308\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:25,757\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 27:=============================>                            (2 + 2) / 4]2021-02-04 17:05:25,857\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:25,873\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 27:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 150us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78cc5a53b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:26,541\tWARNING worker.py:1072 -- WARNING: 20 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:26,553\tWARNING worker.py:1072 -- WARNING: 21 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:26,567\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:26,569\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:26,777\tWARNING worker.py:1072 -- WARNING: 22 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:26,778\tWARNING worker.py:1072 -- WARNING: 22 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:26,784\tWARNING worker.py:1072 -- WARNING: 22 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:26,944\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:27,029\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:27,270\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:27,272\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:27,280\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:27,677\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 32:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 206us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78cc575680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:28,623\tWARNING worker.py:1072 -- WARNING: 23 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:28,624\tWARNING worker.py:1072 -- WARNING: 24 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:28,649\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:28,684\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:28,858\tWARNING worker.py:1072 -- WARNING: 25 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:28,861\tWARNING worker.py:1072 -- WARNING: 25 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:28,864\tWARNING worker.py:1072 -- WARNING: 25 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:29,018\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:29,158\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:29,304\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:29,307\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:29,694\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:29,745\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 37:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 187us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78ac7b1290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:30,559\tWARNING worker.py:1072 -- WARNING: 26 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:30,570\tWARNING worker.py:1072 -- WARNING: 27 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:30,587\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:30,822\tWARNING worker.py:1072 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:30,824\tWARNING worker.py:1072 -- WARNING: 28 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:31,076\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:31,098\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:31,145\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:31,316\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:31,316\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:31,364\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:31,697\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 42:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 168us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78ac39ac20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:32,566\tWARNING worker.py:1072 -- WARNING: 29 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:32,581\tWARNING worker.py:1072 -- WARNING: 30 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:32,587\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:32,632\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:32,809\tWARNING worker.py:1072 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:32,812\tWARNING worker.py:1072 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:32,815\tWARNING worker.py:1072 -- WARNING: 31 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:32,958\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:32,970\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:33,229\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:33,264\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:33,614\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:33,659\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 47:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 174us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f788c693320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "d"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:34,659\tWARNING worker.py:1072 -- WARNING: 32 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:34,670\tWARNING worker.py:1072 -- WARNING: 33 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:34,679\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:34,720\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:34,899\tWARNING worker.py:1072 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:34,900\tWARNING worker.py:1072 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:34,904\tWARNING worker.py:1072 -- WARNING: 34 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:35,061\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:35,146\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:35,305\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:35,348\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:35,677\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:35,699\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 52:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 164us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78ac7b1b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:36,548\tWARNING worker.py:1072 -- WARNING: 35 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:36,551\tWARNING worker.py:1072 -- WARNING: 36 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:36,575\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:36,624\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:36,895\tWARNING worker.py:1072 -- WARNING: 37 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:36,896\tWARNING worker.py:1072 -- WARNING: 37 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:36,898\tWARNING worker.py:1072 -- WARNING: 37 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:37,109\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:37,118\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:37,384\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:37,385\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:37,781\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:37,929\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 57:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 156us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f788c12c7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:38,723\tWARNING worker.py:1072 -- WARNING: 38 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:38,725\tWARNING worker.py:1072 -- WARNING: 39 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:38,788\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:38,792\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:39,001\tWARNING worker.py:1072 -- WARNING: 40 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:39,004\tWARNING worker.py:1072 -- WARNING: 40 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:39,010\tWARNING worker.py:1072 -- WARNING: 40 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:39,356\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:39,364\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:39,448\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:39,492\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:39,838\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:39,845\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 62:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 153us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78844a5440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:40,727\tWARNING worker.py:1072 -- WARNING: 41 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:40,738\tWARNING worker.py:1072 -- WARNING: 42 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:40,748\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:40,788\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:40,982\tWARNING worker.py:1072 -- WARNING: 43 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:40,983\tWARNING worker.py:1072 -- WARNING: 43 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:40,986\tWARNING worker.py:1072 -- WARNING: 43 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:41,124\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:41,254\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:41,400\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:41,448\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:41,823\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:41,853\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 67:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 151us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78840dab00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:42,577\tWARNING worker.py:1072 -- WARNING: 44 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:42,590\tWARNING worker.py:1072 -- WARNING: 45 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:42,597\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:42,597\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:42,828\tWARNING worker.py:1072 -- WARNING: 46 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:42,832\tWARNING worker.py:1072 -- WARNING: 46 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:42,834\tWARNING worker.py:1072 -- WARNING: 46 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:42,978\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:43,097\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:43,358\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:43,711\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 72:>                                                         (0 + 4) / 4]2021-02-04 17:05:43,836\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:43,852\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 72:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 159us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7853f4b9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:44,787\tWARNING worker.py:1072 -- WARNING: 47 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:44,788\tWARNING worker.py:1072 -- WARNING: 48 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:44,820\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:44,820\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:44,860\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:45,023\tWARNING worker.py:1072 -- WARNING: 49 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:45,023\tWARNING worker.py:1072 -- WARNING: 49 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:45,024\tWARNING worker.py:1072 -- WARNING: 49 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:45,030\tWARNING worker.py:1072 -- WARNING: 49 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:45,194\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:45,401\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:45,448\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:45,795\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:45,820\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 77:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 186us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f788c6665f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:46,633\tWARNING worker.py:1072 -- WARNING: 50 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:46,644\tWARNING worker.py:1072 -- WARNING: 51 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:46,677\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:46,912\tWARNING worker.py:1072 -- WARNING: 52 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:46,913\tWARNING worker.py:1072 -- WARNING: 52 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:47,136\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:47,162\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:47,223\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:47,448\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:47,464\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:47,861\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:47,863\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 82:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 154us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7857267320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:48,768\tWARNING worker.py:1072 -- WARNING: 53 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:48,770\tWARNING worker.py:1072 -- WARNING: 54 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:48,786\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:48,824\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:48,824\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:49,023\tWARNING worker.py:1072 -- WARNING: 55 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:49,024\tWARNING worker.py:1072 -- WARNING: 55 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:49,026\tWARNING worker.py:1072 -- WARNING: 55 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:49,029\tWARNING worker.py:1072 -- WARNING: 55 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:49,225\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:49,416\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:49,885\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:49,969\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:49,975\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 87:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 171us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7853d67f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:50,839\tWARNING worker.py:1072 -- WARNING: 56 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:50,850\tWARNING worker.py:1072 -- WARNING: 57 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:50,863\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:50,863\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:50,864\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:51,082\tWARNING worker.py:1072 -- WARNING: 58 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:51,083\tWARNING worker.py:1072 -- WARNING: 58 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:51,084\tWARNING worker.py:1072 -- WARNING: 58 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:51,085\tWARNING worker.py:1072 -- WARNING: 58 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:51,250\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:51,472\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:51,473\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:51,508\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:51,900\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 92:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 151us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78538c6560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:52,747\tWARNING worker.py:1072 -- WARNING: 59 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:52,759\tWARNING worker.py:1072 -- WARNING: 60 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:53,005\tWARNING worker.py:1072 -- WARNING: 61 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:53,187\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:53,269\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:53,302\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:53,362\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:53,491\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:53,491\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:53,491\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:53,892\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 97:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 193us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78cc5a58c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:54,919\tWARNING worker.py:1072 -- WARNING: 62 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:54,920\tWARNING worker.py:1072 -- WARNING: 63 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:54,936\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:54,936\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:55,160\tWARNING worker.py:1072 -- WARNING: 64 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:55,161\tWARNING worker.py:1072 -- WARNING: 64 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:55,164\tWARNING worker.py:1072 -- WARNING: 64 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:55,327\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:55,396\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:55,587\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:55,628\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:55,974\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:55,989\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 102:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 222us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f788c693c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:56,805\tWARNING worker.py:1072 -- WARNING: 65 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:56,812\tWARNING worker.py:1072 -- WARNING: 66 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:56,868\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:57,059\tWARNING worker.py:1072 -- WARNING: 67 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:57,066\tWARNING worker.py:1072 -- WARNING: 67 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:57,225\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:57,263\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:57,334\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:57,584\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:57,973\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:57,991\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:58,015\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 107:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 268us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7856a5a440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:05:58,842\tWARNING worker.py:1072 -- WARNING: 68 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:58,854\tWARNING worker.py:1072 -- WARNING: 69 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:58,870\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:58,870\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:58,916\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:58,916\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:59,085\tWARNING worker.py:1072 -- WARNING: 70 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:59,086\tWARNING worker.py:1072 -- WARNING: 70 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:59,088\tWARNING worker.py:1072 -- WARNING: 70 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:59,089\tWARNING worker.py:1072 -- WARNING: 70 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:59,089\tWARNING worker.py:1072 -- WARNING: 70 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:05:59,459\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:59,862\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:59,873\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:05:59,878\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 112:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 1ms/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7852bc8dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:00,764\tWARNING worker.py:1072 -- WARNING: 71 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:00,778\tWARNING worker.py:1072 -- WARNING: 72 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:00,788\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:00,828\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:00,832\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:01,013\tWARNING worker.py:1072 -- WARNING: 73 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:01,018\tWARNING worker.py:1072 -- WARNING: 73 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:01,022\tWARNING worker.py:1072 -- WARNING: 73 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:01,022\tWARNING worker.py:1072 -- WARNING: 73 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:01,158\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:01,388\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:01,440\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:01,787\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:01,798\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 117:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 163us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f785273b680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "d"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:02,622\tWARNING worker.py:1072 -- WARNING: 74 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:02,623\tWARNING worker.py:1072 -- WARNING: 75 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:02,636\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:02,676\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:02,866\tWARNING worker.py:1072 -- WARNING: 76 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:02,868\tWARNING worker.py:1072 -- WARNING: 76 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:02,869\tWARNING worker.py:1072 -- WARNING: 76 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:03,032\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:03,141\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:03,324\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:03,328\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:03,765\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:03,783\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 122:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 169us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78512223b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:04,517\tWARNING worker.py:1072 -- WARNING: 77 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:04,517\tWARNING worker.py:1072 -- WARNING: 78 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:04,534\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:04,563\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:04,596\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:04,771\tWARNING worker.py:1072 -- WARNING: 79 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:04,775\tWARNING worker.py:1072 -- WARNING: 79 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:04,776\tWARNING worker.py:1072 -- WARNING: 79 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:04,777\tWARNING worker.py:1072 -- WARNING: 79 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:04,921\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:05,163\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:05,212\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:05,559\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:05,587\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 127:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 268us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78511cdcb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:06,717\tWARNING worker.py:1072 -- WARNING: 80 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:06,730\tWARNING worker.py:1072 -- WARNING: 81 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:06,788\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:06,980\tWARNING worker.py:1072 -- WARNING: 82 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:06,984\tWARNING worker.py:1072 -- WARNING: 82 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "[Stage 131:>                                                        (0 + 4) / 4]2021-02-04 17:06:07,259\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:07,259\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:07,262\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:07,452\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:07,897\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:08,042\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 132:==============>                                          (1 + 3) / 4]2021-02-04 17:06:08,048\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 132:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 164us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f788c63af80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:08,776\tWARNING worker.py:1072 -- WARNING: 83 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:08,779\tWARNING worker.py:1072 -- WARNING: 84 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:08,794\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:08,794\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:08,840\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:09,013\tWARNING worker.py:1072 -- WARNING: 85 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:09,014\tWARNING worker.py:1072 -- WARNING: 85 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:09,015\tWARNING worker.py:1072 -- WARNING: 85 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:09,019\tWARNING worker.py:1072 -- WARNING: 85 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:09,193\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:09,435\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:09,456\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:09,903\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:09,937\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 137:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 157us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7855a629e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "d"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:10,748\tWARNING worker.py:1072 -- WARNING: 86 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:10,762\tWARNING worker.py:1072 -- WARNING: 87 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:10,769\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:10,792\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:11,026\tWARNING worker.py:1072 -- WARNING: 88 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:11,031\tWARNING worker.py:1072 -- WARNING: 88 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:11,031\tWARNING worker.py:1072 -- WARNING: 88 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:11,238\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:11,257\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:11,446\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:11,488\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:11,880\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:11,902\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 142:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 150us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7853357680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:12,836\tWARNING worker.py:1072 -- WARNING: 89 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:12,837\tWARNING worker.py:1072 -- WARNING: 90 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:12,852\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:12,852\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:13,093\tWARNING worker.py:1072 -- WARNING: 91 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:13,094\tWARNING worker.py:1072 -- WARNING: 91 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:13,101\tWARNING worker.py:1072 -- WARNING: 91 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:13,319\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:13,331\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:13,561\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:13,561\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:14,016\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:14,062\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 147:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 192us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7852ec83b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:14,944\tWARNING worker.py:1072 -- WARNING: 92 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:14,946\tWARNING worker.py:1072 -- WARNING: 93 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:14,960\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:15,000\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:15,191\tWARNING worker.py:1072 -- WARNING: 94 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:15,192\tWARNING worker.py:1072 -- WARNING: 94 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:15,195\tWARNING worker.py:1072 -- WARNING: 94 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:15,348\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:15,374\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:15,701\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:15,717\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:16,142\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:16,174\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 152:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 181us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784fed2f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:17,121\tWARNING worker.py:1072 -- WARNING: 95 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:17,123\tWARNING worker.py:1072 -- WARNING: 96 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:17,137\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:17,137\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:17,398\tWARNING worker.py:1072 -- WARNING: 97 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:17,399\tWARNING worker.py:1072 -- WARNING: 97 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:17,399\tWARNING worker.py:1072 -- WARNING: 97 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:17,565\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 156:>                                                        (0 + 4) / 4]2021-02-04 17:06:17,654\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:17,823\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:17,823\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:18,242\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:18,275\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 172us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784f2377a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "d"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:19,262\tWARNING worker.py:1072 -- WARNING: 98 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:19,274\tWARNING worker.py:1072 -- WARNING: 99 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:19,328\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:19,548\tWARNING worker.py:1072 -- WARNING: 100 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:19,551\tWARNING worker.py:1072 -- WARNING: 100 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:19,788\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:19,793\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:19,796\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:20,031\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:20,034\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:20,455\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:20,469\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 162:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 430us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784f1e2d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:21,721\tWARNING worker.py:1072 -- WARNING: 101 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:21,742\tWARNING worker.py:1072 -- WARNING: 102 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:21,750\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:21,792\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:21,804\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:21,979\tWARNING worker.py:1072 -- WARNING: 103 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:21,982\tWARNING worker.py:1072 -- WARNING: 103 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:21,982\tWARNING worker.py:1072 -- WARNING: 103 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:21,985\tWARNING worker.py:1072 -- WARNING: 103 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:22,136\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:22,396\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:22,428\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:22,854\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:22,872\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 167:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 175us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f788c6930e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:23,739\tWARNING worker.py:1072 -- WARNING: 104 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:23,747\tWARNING worker.py:1072 -- WARNING: 105 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:24,064\tWARNING worker.py:1072 -- WARNING: 106 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:24,227\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:24,341\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:24,375\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:24,382\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:24,608\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:24,608\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:24,612\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:25,062\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 172:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 181us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78560ea7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:25,975\tWARNING worker.py:1072 -- WARNING: 107 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:25,989\tWARNING worker.py:1072 -- WARNING: 108 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:25,996\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:26,036\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:26,253\tWARNING worker.py:1072 -- WARNING: 109 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:26,256\tWARNING worker.py:1072 -- WARNING: 109 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:26,257\tWARNING worker.py:1072 -- WARNING: 109 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:26,395\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:26,576\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:26,678\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:26,683\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:27,116\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:27,122\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 177:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 202us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f785239d440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "d"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:28,090\tWARNING worker.py:1072 -- WARNING: 110 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:28,093\tWARNING worker.py:1072 -- WARNING: 111 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:28,116\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:28,116\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:28,347\tWARNING worker.py:1072 -- WARNING: 112 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:28,347\tWARNING worker.py:1072 -- WARNING: 112 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:28,348\tWARNING worker.py:1072 -- WARNING: 112 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:28,589\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 181:>                                                        (0 + 4) / 4]2021-02-04 17:06:28,663\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:28,755\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:28,756\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:28,759\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:29,182\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 182:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 353us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78517ceb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:30,124\tWARNING worker.py:1072 -- WARNING: 113 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:30,136\tWARNING worker.py:1072 -- WARNING: 114 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:30,148\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:30,150\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:30,416\tWARNING worker.py:1072 -- WARNING: 115 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:30,419\tWARNING worker.py:1072 -- WARNING: 115 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:30,422\tWARNING worker.py:1072 -- WARNING: 115 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:30,605\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 186:>                                                        (0 + 4) / 4]2021-02-04 17:06:30,656\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:30,865\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:30,870\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:31,381\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:31,400\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 187:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 163us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784e744c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:32,257\tWARNING worker.py:1072 -- WARNING: 116 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:32,270\tWARNING worker.py:1072 -- WARNING: 117 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:32,352\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:32,542\tWARNING worker.py:1072 -- WARNING: 118 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:32,546\tWARNING worker.py:1072 -- WARNING: 118 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:32,716\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:32,744\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:32,827\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:33,010\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:33,011\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:33,011\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:33,485\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 192:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 163us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784daf7560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:34,387\tWARNING worker.py:1072 -- WARNING: 119 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:34,399\tWARNING worker.py:1072 -- WARNING: 120 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:34,418\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:34,456\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:34,751\tWARNING worker.py:1072 -- WARNING: 121 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:34,754\tWARNING worker.py:1072 -- WARNING: 121 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:34,755\tWARNING worker.py:1072 -- WARNING: 121 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:34,833\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:34,835\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:35,144\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:35,192\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:35,614\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:35,628\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 197:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 164us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784ce68290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:36,529\tWARNING worker.py:1072 -- WARNING: 122 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:36,541\tWARNING worker.py:1072 -- WARNING: 123 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:36,557\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:36,566\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:36,807\tWARNING worker.py:1072 -- WARNING: 124 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:36,811\tWARNING worker.py:1072 -- WARNING: 124 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:36,813\tWARNING worker.py:1072 -- WARNING: 124 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:37,000\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 201:>                                                        (0 + 4) / 4]2021-02-04 17:06:37,147\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:37,351\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:37,353\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:37,800\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 202:==============>                                          (1 + 3) / 4]2021-02-04 17:06:37,857\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 202:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 192us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7855a13050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:39,162\tWARNING worker.py:1072 -- WARNING: 125 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:39,173\tWARNING worker.py:1072 -- WARNING: 126 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:39,191\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:39,201\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:39,459\tWARNING worker.py:1072 -- WARNING: 127 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:39,467\tWARNING worker.py:1072 -- WARNING: 127 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:39,467\tWARNING worker.py:1072 -- WARNING: 127 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:39,638\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:39,817\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:39,972\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:40,433\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:40,484\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:40,489\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 207:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78516d34d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 179us/step\n",
      "s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:41,575\tWARNING worker.py:1072 -- WARNING: 128 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:41,590\tWARNING worker.py:1072 -- WARNING: 129 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:41,656\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:41,880\tWARNING worker.py:1072 -- WARNING: 130 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:41,882\tWARNING worker.py:1072 -- WARNING: 130 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:42,097\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:42,155\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:42,222\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:42,333\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:42,333\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:42,376\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:42,376\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 212:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 153us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784fe20ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:43,728\tWARNING worker.py:1072 -- WARNING: 131 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:43,748\tWARNING worker.py:1072 -- WARNING: 132 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:43,765\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:43,796\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:43,996\tWARNING worker.py:1072 -- WARNING: 133 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:43,996\tWARNING worker.py:1072 -- WARNING: 133 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:43,997\tWARNING worker.py:1072 -- WARNING: 133 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:44,261\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:44,264\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:44,472\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:44,473\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:44,493\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:44,878\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 217:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 147us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7854155b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "i"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:45,839\tWARNING worker.py:1072 -- WARNING: 134 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:45,866\tWARNING worker.py:1072 -- WARNING: 135 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:45,872\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:46,191\tWARNING worker.py:1072 -- WARNING: 136 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:46,193\tWARNING worker.py:1072 -- WARNING: 136 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:46,328\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:46,355\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:46,368\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:46,689\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:47,168\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:47,204\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:47,208\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 222:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 220us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7850582c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:48,119\tWARNING worker.py:1072 -- WARNING: 137 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:48,143\tWARNING worker.py:1072 -- WARNING: 138 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:48,157\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:48,220\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:48,478\tWARNING worker.py:1072 -- WARNING: 139 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:48,479\tWARNING worker.py:1072 -- WARNING: 139 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:48,479\tWARNING worker.py:1072 -- WARNING: 139 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:48,568\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:48,572\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:48,904\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:48,904\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:48,906\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:49,392\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 227:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 170us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7850141560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:50,353\tWARNING worker.py:1072 -- WARNING: 140 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:50,364\tWARNING worker.py:1072 -- WARNING: 141 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:50,381\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:50,381\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:50,669\tWARNING worker.py:1072 -- WARNING: 142 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:50,669\tWARNING worker.py:1072 -- WARNING: 142 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:50,671\tWARNING worker.py:1072 -- WARNING: 142 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:50,847\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:50,989\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:51,102\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:51,582\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 232:==============>                                          (1 + 3) / 4]2021-02-04 17:06:51,697\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:51,714\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 249us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784c965290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:52,633\tWARNING worker.py:1072 -- WARNING: 143 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:52,661\tWARNING worker.py:1072 -- WARNING: 144 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:52,672\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:52,693\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:52,909\tWARNING worker.py:1072 -- WARNING: 145 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:52,915\tWARNING worker.py:1072 -- WARNING: 145 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:52,918\tWARNING worker.py:1072 -- WARNING: 145 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:53,111\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:53,167\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:53,342\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:53,342\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:53,384\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:53,771\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 237:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 1ms/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784b3d99e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:54,701\tWARNING worker.py:1072 -- WARNING: 146 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:54,716\tWARNING worker.py:1072 -- WARNING: 147 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:54,733\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:54,733\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:55,066\tWARNING worker.py:1072 -- WARNING: 148 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:55,066\tWARNING worker.py:1072 -- WARNING: 148 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:55,072\tWARNING worker.py:1072 -- WARNING: 148 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:55,251\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 241:>                                                        (0 + 4) / 4]2021-02-04 17:06:55,318\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:55,508\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:55,508\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:56,011\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:56,023\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 242:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 151us/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f784a73d320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:56,948\tWARNING worker.py:1072 -- WARNING: 149 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:56,961\tWARNING worker.py:1072 -- WARNING: 150 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:56,977\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:57,259\tWARNING worker.py:1072 -- WARNING: 151 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:57,260\tWARNING worker.py:1072 -- WARNING: 151 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "[Stage 246:>                                                        (0 + 4) / 4]2021-02-04 17:06:57,518\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:57,548\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:57,565\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:57,736\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:57,744\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:57,776\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:06:58,203\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 247:==========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m \r",
      "1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1/1 [==============================] - 0s 1ms/step\n",
      "\u001b[2m\u001b[36m(pid=17535)\u001b[0m WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7849b86f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "d"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:06:59,856\tWARNING worker.py:1072 -- WARNING: 152 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:59,864\tWARNING worker.py:1072 -- WARNING: 153 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:06:59,936\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:07:00,150\tWARNING worker.py:1072 -- WARNING: 154 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:07:00,154\tWARNING worker.py:1072 -- WARNING: 154 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2021-02-04 17:07:00,430\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "[Stage 251:>                                                        (0 + 4) / 4]2021-02-04 17:07:00,487\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:07:00,504\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:07:00,681\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2021-02-04 17:07:00,682\tWARNING worker.py:792 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8607e182ac25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#sample_shards = sample_shards.transform_shard(transform_to_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analytics-zoo/pyzoo/zoo/orca/learn/tf2/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, batch_size, verbose, steps, callbacks, data_config, feature_cols)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_predict_xshards_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mpred_shards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_spark_xshards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_predict_xshards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analytics-zoo/pyzoo/zoo/orca/learn/tf2/estimator.py\u001b[0m in \u001b[0;36m_predict_spark_xshards\u001b[0;34m(self, xshards, params)\u001b[0m\n\u001b[1;32m    276\u001b[0m                                                                \u001b[0mtransform_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                                                                gang_scheduling=False)\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mspark_xshards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_shards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_spark_xshards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mspark_xshards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analytics-zoo/pyzoo/zoo/orca/data/shard.py\u001b[0m in \u001b[0;36mto_spark_xshards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_spark_xshards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mray_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_spark_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analytics-zoo/pyzoo/zoo/orca/data/shard.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rdd, transient)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analytics-zoo/pyzoo/zoo/orca/data/shard.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from zoo.orca.data import XShards\n",
    "\n",
    "def transform_to_dict(data):\n",
    "    return {\"x\": data[0]}\n",
    "\n",
    "\n",
    "for epoch in range(1, 60):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    stats = est.fit(train_data_creator, \n",
    "                    epochs=1,\n",
    "                    batch_size=batch_size,\n",
    "                    steps_per_epoch=200278 // batch_size,\n",
    "                    verbose=0)\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "                \n",
    "            sample_shards = XShards.partition({\"x\": sampled}, num_shards=1)\n",
    "            #sample_shards = sample_shards.transform_shard(transform_to_dict)\n",
    "\n",
    "            preds = est.predict(sample_shards).collect()\n",
    "            \n",
    "            next_index = sample(preds[0]['prediction'][0], temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "est.shutdown()\n",
    "#stop_orca_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
