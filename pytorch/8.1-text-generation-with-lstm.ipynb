{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extended-algorithm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-reggae",
   "metadata": {},
   "source": [
    "To start using zoo.orca, we need to first initialize orca context. Here we specify local or distributed mode. In this example, we choose the local mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.orca import init_orca_context, stop_orca_context\n",
    "from zoo.orca import OrcaContext\n",
    "\n",
    "# recommended to set it to True when running Analytics Zoo in Jupyter notebook. \n",
    "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
    "\n",
    "cluster_mode = \"local\"\n",
    "\n",
    "if cluster_mode == \"local\":\n",
    "    init_orca_context(cores=1, memory=\"2g\")   # run in local mode\n",
    "elif cluster_mode == \"k8s\":\n",
    "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
    "elif cluster_mode == \"yarn\":\n",
    "    init_orca_context(\n",
    "        cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
    "        driver_memory=\"10g\", driver_cores=1,\n",
    "        conf={\"spark.rpc.message.maxSize\": \"1024\",\n",
    "              \"spark.task.maxFailures\": \"1\",\n",
    "              \"spark.driver.extraJavaOptions\": \"-Dbigdl.failure.retryTimes=1\"})   # run on Hadoop YARN cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-brush",
   "metadata": {},
   "source": [
    "# Text generation with LSTM\n",
    "This notebook contains the code samples found in Chapter 8, Section 1 of Deep Learning with Python. Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "[...]\n",
    "\n",
    "## Implementing character-level LSTM text generation\n",
    "Let's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the English language.\n",
    "\n",
    "## Preparing the data\n",
    "Let's start by downloading the corpus and converting it to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protected-associate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./nietzsche.txt\n",
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets.utils import download_url\n",
    "\n",
    "download_url(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", '.')\n",
    "text = open('./nietzsche.txt').read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-hanging",
   "metadata": {},
   "source": [
    "Next, we will extract partially-overlapping sequences of length maxlen, one-hot encode them and pack them in a 3D Numpy array `x` of shape `(sequences, maxlen, unique_characters)`. Simultaneously, we prepare a array `y` containing the corresponding targets: the one-hot encoded characters that come right after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "modern-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique characters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen), dtype=np.int32)\n",
    "y = np.zeros((len(sentences)), dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t] = char_indices[char]\n",
    "    y[i] = char_indices[next_chars[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-sugar",
   "metadata": {},
   "source": [
    "\n",
    "## Building the network\n",
    "Our network is a single LSTM layer followed by a Dense classifier and softmax over all possible characters. But let us note that recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in recent times. Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sapphire-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, maxlen, chars):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(chars), maxlen)\n",
    "        self.lstm = nn.LSTM(maxlen, 128)\n",
    "        self.fc = nn.Linear(128, len(chars))\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        self.chars = chars\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, vocab_size = x.size()\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        h0 = torch.randn(1, vocab_size, 128)\n",
    "        c0 = torch.randn(1, vocab_size, 128)\n",
    "        (x, _) = self.lstm(x, (h0, c0))\n",
    "        #print(x.size())\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        return F.log_softmax(self.fc(x[:, -1, :]), dim=-1)\n",
    "        \n",
    "model = LSTMNet(maxlen, chars)\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "transparent-breakfast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNet(\n",
      "  (embedding): Embedding(57, 60)\n",
      "  (lstm): LSTM(60, 128)\n",
      "  (fc): Linear(in_features=128, out_features=57, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-cartridge",
   "metadata": {},
   "source": [
    "We also create a `data_creator` function that help us import the data into the orca estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "graduate-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def data_loader_creator(data, targets, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Transforms data and targets from np.array to torch.utils.data.DataLoader,\n",
    "    and shuffle the dataset if `shuffle` is set to true.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_tensor = torch.LongTensor(data)\n",
    "    targets_tensor = torch.LongTensor(targets)\n",
    "    dataset = TensorDataset(data_tensor, targets_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "parental-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_loader_creator(x, y, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-train",
   "metadata": {},
   "source": [
    "Training the language model and sampling from it\n",
    "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
    "\n",
    "* 1) Drawing from the model a probability distribution over the next character given the text available so far\n",
    "* 2) Reweighting the distribution to a certain \"temperature\"\n",
    "* 3) Sampling the next character at random according to the reweighted distribution\n",
    "* 4) Adding the new character at the end of the available text\n",
    "\n",
    "This is the code we use to reweight the original probability distribution coming out of the model, and draw a character index from it (the \"sampling function\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enclosed-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = preds / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-cassette",
   "metadata": {},
   "source": [
    "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of temperature in the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hindu-reach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasAccuracy\n",
      "creating: createTorchLoss\n",
      "creating: createTorchOptim\n",
      "creating: createTorchModel\n",
      "creating: createEstimator\n"
     ]
    }
   ],
   "source": [
    "from zoo.orca.learn.pytorch import Estimator \n",
    "from zoo.orca.learn.metrics import Accuracy\n",
    "\n",
    "est = Estimator.from_torch(model=model, optimizer=opt, loss=criterion, metrics=[Accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-semiconductor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from zoo.orca.data import XShards\n",
    "import random\n",
    "\n",
    "batch_size=128\n",
    "output = \"\"\n",
    "for epoch in range(1, 10):\n",
    "    #print('epoch', epoch)\n",
    "    output += \"epoch \" + str(epoch) + '\\n'\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    est.fit(data=train_loader, epochs=epoch)\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    #print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "    output += '--- Generating with seed: \"' + str(generated_text) + '\"\\n'\n",
    "    trained_model = est.get_model()\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        #print('------ temperature:', temperature)\n",
    "        output += '------ temperature: ' + str(temperature) + '\\n'\n",
    "        #sys.stdout.write(generated_text)\n",
    "        output += generated_text\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t] = char_indices[char]\n",
    "                \n",
    "            #sample_shards = XShards.partition({\"x\": sampled}, num_shards=1)\n",
    "            #preds = est.predict(sample_shards).collect()\n",
    "            #next_index = sample(preds[0]['prediction'][0], temperature)\n",
    "            \n",
    "            \n",
    "            next_index = sample(trained_model(torch.LongTensor(sampled)).detach().numpy()[0], temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            #sys.stdout.write(next_char)\n",
    "            output += next_char\n",
    "            #sys.stdout.flush()\n",
    "        #print()\n",
    "        output += '\\n'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "owned-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "--- Generating with seed: \" the\n",
      "origin of ideas.\n",
      "\n",
      "21. the causa sui is the best self-co\"\n",
      "------ temperature: 0.2\n",
      " the\n",
      "origin of ideas.\n",
      "\n",
      "21. the causa sui is the best self-co ind ane the we the athis and sthe al an t thin woure t in the ind an at thinceres thin and and ind as athind thr in an thorere in in and the icof as an the anthen and the the he athe ond in the there and wed f ce ind ind athe man an ande onde and hin ou=; the t the an me and thand t inen t is itin the win on the ind and an t thin the ind s an han alind in ind an ind t wisthen thend ce icon ind on\n",
      "------ temperature: 0.5\n",
      " s an han alind in ind an ind t wisthen thend ce icon ind ond anern ourondstere arendourond the is ange t itheind nd thof theres fucildt aceerene ine k, man icesuswerd t mes ond ind thin and anasilond aner anino inindinof ind cofat ar whar is d oue ithire aredere hea ice ntlerer hais an theshere ble t me thers therthin s a ald is tit oun hatayer land ces thour at is in d mo menther o at n and be in oughen he f medokern alore is ond ilisur th st iones ichou\n",
      "------ temperature: 1.0\n",
      "n oughen he f medokern alore is ond ilisur th st iones ichouanduceprid susucofuboret\n",
      "azoricomed  daner; areanveldie: ancofe odemerty. pane cothase,.=6ctecopig coornd tusw ang\n",
      "wooffoofr\n",
      "edey an phowig ant t-7titone  me tedene pio d\n",
      "mpinthineme d he a acod, h se incimoud prather\n",
      "tist cl ousqzzd m haso alerstiomsupee thopr m, sak cin, an\n",
      "andak tofimemerundmpo\n",
      "=inechespmiveimo oumasin, ay\"0xnd thedntisouren ilomss\n",
      "co t coul,\n",
      "\n",
      "\n",
      "\n",
      "rs aris: whinds imp8jenanghe, ve\n",
      "------ temperature: 1.2\n",
      "souren ilomss\n",
      "co t coul,\n",
      "\n",
      "\n",
      "\n",
      "rs aris: whinds imp8jenanghe, ve w.-phoy ydgis\n",
      "\n",
      "itheertsheches wnl dy=6deredidoph: 2jeden, l\n",
      "14] warist cl m rdindito haizot m. ait fithist-ir\n",
      "ishedlyy hu-ethrsthhe cedd\n",
      "l s f incre\n",
      "adcev543jim e (juso,\"4865863).\n",
      " ser;\"5uuprt. nte i if ha g, pur\n",
      "fe g, mendn atothalisthenod s rn (ialdo\n",
      "ssccitangotinyk546. c e theno, uenges, urerisheshef g ldgis anis suphas; \"=, tesqrpasepus ki d-qlmo tsufofrn\n",
      "ss jonefouag nemy, ospes\n",
      "anwheredinur\n",
      "epoch 2\n",
      "--- Generating with seed: \"erated upon mankind. the more similar, the more ordinary\n",
      "peo\"\n",
      "------ temperature: 0.2\n",
      "erated upon mankind. the more similar, the more ordinary\n",
      "peonere ane thind thuthin an he in there the athe in athe indithe (and athe there as athe the ale alin in aindin the ind an t atherere whe ave win on the ore and arerd t in ande the t in athe athe ice and as than ind thind (athe al the the an in thend as the an an as and the the t arere the hind the an in ane (th t thond in ince aing is ind arere whan s t ind there in s ite he an her athe is and wis \n",
      "------ temperature: 0.5\n",
      "arere whan s t ind there in s ite he an her athe is and wis ig: alog areend thintho on pen inst isus in ulearentherot ly inertoured ofofond\n",
      "ace this in anthand at e wh is of t\n",
      "tin ace ar ons hin indis coren iene this, ardindin ale asthecisthe t alinon heve ing ire ar an hns mathing ane hall in whe d tis in alyise the me in, ans thisthine t ghere fif co ave w avondinge one we han isther wn arywhind s a he bere secalofunere t cked s atis avecof d theare alal\n",
      "------ temperature: 1.0\n",
      " s a he bere secalofunere t cked s atis avecof d theare alalequs e hodiovarombiore sque velfumpiskerend osthepl\n",
      "ineskey d usith, an thealy mpr--icq\"erstherlid\n",
      " bosistuelas onstilfannd aythy achonan w\n",
      "allyiof\n",
      "rursen alineut iner\n",
      "t tiren\n",
      "g g-buers owon, on fo d as larit woure seduhind ner, fo. [12.\n",
      "ithin thoppaled ays, tod\n",
      "0 weren, galetingeregimee peaicofourynes-n?--\n",
      "ofolwliethind int tibeceittous d thentle wa lsce tyreasshig\" torotesnir exia anobort an\"nel\n",
      "------ temperature: 1.2\n",
      "d thentle wa lsce tyreasshig\" torotesnir exia anobort an\"nel venge s. owomine io\n",
      "ie ald. cir c\n",
      "pulery33rofg:-winje herisusisu iean) t, imbld i \"ff rmprenhe bouti gal,\n",
      "\n",
      "d\n",
      "hae\n",
      "tharend t, d thid w nisisurliferit unal,\n",
      "r hny-l i pat\n",
      "r\n",
      "afony gkuprapibsuf h huparirreche moul be'sial avofr heviveloforlumeracasfoupp\n",
      "dmusof ke aci inaluabe adu?\n",
      "\n",
      "f bctheph pt prinare atince\"rncor, wh ony f\n",
      "grafoutikid (frl lls sug wowiang o, m-hes ifeserpt\n",
      "oseselpent siser t, u l he\n",
      "epoch 3\n",
      "--- Generating with seed: \"uch a\n",
      "distinction to have one's own antipodes!\n",
      "\n",
      "49. that whi\"\n",
      "------ temperature: 0.2\n",
      "uch a\n",
      "distinction to have one's own antipodes!\n",
      "\n",
      "49. that whithe an athe hes ind in thondis athe and an in the is an therind and in asth he a the ande hin and the an athin athathe t ithe an he athe in t the the is ine in an ane th t t (the at in there athin he ind athe and ine t ind thin he ithere me the the athe an there an in an an ace athe ind in ind ans the the is the anthend and the in anend and the ales aro a thind ind he the he an the he as he the hi\n",
      "------ temperature: 0.5\n",
      "nd the ales aro a thind ind he the he an the he as he the his andeathereree in themer alacere in an the t is one henat be is athe fo cestithen henow athas hond ane int ind here and\n",
      "in mas in wone hes cert ithe m s athice t he infous a wind auronecomone t t t t in r athiothe and hitondirmptind ieron inthe in s orerece he out indoutheroum ithe t ate ce, t therend is and ofe met at d t iser onomaghe ine athe whasen this is: ond it t he the ithon oresend o win\n",
      "------ temperature: 1.0\n",
      "ine athe whasen this is: ond it t he the ithon oresend o winerco caprcequonssiod fendss\n",
      " oi muand for wa s a thh wheat raclost ceaked-hmeng, ex whirorichowhe han\n",
      "thalan\"e f wenwonecanen t\n",
      "\n",
      "sarofte uly\n",
      "\n",
      "h hond avegy cepaviondlthun bond tia mongh g\n",
      "1.\n",
      "pher pis is he h t flillilencow maninld\n",
      "payat t anl helumenh de eered he the d anguonsitor igssugsouitstatibterys; ld ir llimos nendve cecisthiald iced cand. ore id notely; ct. plioniar'sthed\n",
      "\n",
      "ala asnerd mppisp\n",
      "------ temperature: 1.2\n",
      "d cand. ore id notely; ct. plioniar'sthed\n",
      "\n",
      "ala asnerd mppisprabious llo unk duoucir (s:\n",
      "ilang y-peryly gis; pse\n",
      "e leat s when s h\n",
      "henven at,-(ilmita s cas\n",
      "1opow the\n",
      "wlo wode bosstaliofewiti tichell wthecuti bfevofr werigss\n",
      "athewnging aoucesth in auc n duceres\n",
      "=\"99=agnassogmpoivongoffe nsmueman the blis rd hut chin sy ophes\n",
      "ginsifiecice,\n",
      "h atolace m o hong snjusupicala me. qurthatif ts\n",
      "adndundeff, ugal su mb by?n,icaceindit (arcates tre dug\n",
      "thbrofayuveue w\n",
      "\n",
      "epoch 4\n",
      "--- Generating with seed: \" men. he aroused first disfavor, then suspicion,\n",
      "became grad\"\n",
      "------ temperature: 0.2\n",
      " men. he aroused first disfavor, then suspicion,\n",
      "became grade thin the and an isthe the he the ang's and an an the ind and and in t ange ind and an the andin there and thin an ind thin is an an ind pe th hind pin there in the hend alere and he an he the he an an and the the and t t the t and the are in of and there ind and ithe in aly ond the and and an and the o onend t hend and the as ind ind in t in in andin th in athe the and an iche ind ales and an th\n",
      "------ temperature: 0.5\n",
      " t in in andin th in athe the and an iche ind ales and an thewh, o at and alon itho thin an wonesthe ond one hearent e aneritouciof po tice the cth are o strens bande ischellqunio athe ind ndalit aterind angon as aner o an of thhend athenqursty\n",
      "ant itare an ancoure ft an hi thher mime om ofongt there t heiticin ititug te ither orinsthe and in the? inthe anis in r t inde s thenn the tin as is n a at ther aninand hene acot onend t and ag ttich ablis asman nt\n",
      "------ temperature: 1.0\n",
      "t ther aninand hene acot onend t and ag ttich ablis asman nt ain mel gern f ivithe tat udipenoots alenon t g ansnt boly h s, ac nthitenth mmatl whe ab our s theainouin\n",
      "is hongior! aiveriakeras, vouie t\n",
      "foseve?'srul ioute hichilig soserealllusid; t f,\n",
      "alise at ngrleve? d ty icag houreresps---------ecat pt tent whhio math s whimaigtsitles isindens\n",
      "ngt t wes, e) heatulilty e bbpatss\n",
      " al). te cised anideedid.[0)\" pect t? at s t fes t whourureng-----whellirasou\n",
      "------ temperature: 1.2\n",
      "ideedid.[0)\" pect t? at s t fes t whourureng-----whellirasoustoe\n",
      "b whly acusc, h antorise hiencicoreintheasue seatuno cepuge rpoundsped e's me arthe ph anwes therint aknaingerne fthangercuporeigrneris witrse tacheererling, gns\n",
      "es d ee aithe ofat sthenst anaversory o hendees e, m peces ijolsospowioncaw atoke athit itepegfo cheriton yute res ampt\n",
      "some  isuavero asene\n",
      "aptodopifrlhathes, aneevery\n",
      "irolf hacsofr\"nuphr; ad\n",
      "ouho oexof tofurong5hinprauthatelisme ff\n",
      "epoch 5\n",
      "--- Generating with seed: \"re is still feeble and\n",
      "uncertain, so that it could be easily\"\n",
      "------ temperature: 0.2\n",
      "re is still feeble and\n",
      "uncertain, so that it could be easily ang ise in ind an in an whe the an s ond t ond and andis t an t t is an the he t inde the t on and t t ind ane in in an the in in s and t ind the t the an the an an in the ind and an and win it t the an ang athe t the an athe an the an t and the and an t an the at is te is athe t an and an t the t the the an t the ay as the t and the and athe the an the in indis the the the on and he is ale ce of\n",
      "------ temperature: 0.5\n",
      " athe the an the in indis the the the on and he is ale ce of athace f o obures ind athas tin it t aneliocond al the angrer lithithan athin mang orithe whiconden ouns t alindean he inithe athe in he isen ured stinow ad whin s m the on the ice is id ther tim!-ane mainsutten t thangr the it ont indelit in ss inthe s f or texpuin ce ved ind tourisue ifitisthe wind mangane d tans and it pre oule on t asice whe in s, isess atict s cofo ha t athe tslis and anan i\n",
      "------ temperature: 1.0\n",
      "sice whe in s, isess atict s cofo ha t athe tslis and anan iguplisouarothe inceved the watouthe n s glfllintrexorisooiay fly, onfron an thererexe t bedntheninig pro tha oson itresines tra h,\n",
      "\n",
      "ong aneco l t rt te nsut ild cinn mishinay, aneerenw  e ashine\"--induar,\n",
      "tharenfa oneeposhin sue, warhe wethexer mo wan. d \"rve henofveasth eanolimapr, is\n",
      "\n",
      "cinceothinsinkiouang!\"=--therndusocuroror whancineeranycithesy, he r asonousid ats tuchint t f ltoulelly). ancoo\n",
      "------ temperature: 1.2\n",
      "anycithesy, he r asonousid ats tuchint t f ltoulelly). ancoong ant\"ant uis\n",
      "tlutupmend tst\n",
      "strtibrs pr  howhe n owh, s ilon picivee abey ( (tug,\n",
      "an, baghmil min\n",
      "ston helkes uguss, heoro \"whero fle e pyc nd \"je\n",
      "pever hens'sexhewisial h\n",
      "d bate clartodnstuleng : isst otexceathetha for fiewimblentigeale theelan, io iacachuchile--m.\n",
      "a ond mpe were (ungn, wotre pin ondifunenas phuemss. e icobltin\n",
      "ad\n",
      "a im whiriorge, mast tat!\"! ncereakg earothewhev---t canio oved \n",
      "epoch 6\n",
      "--- Generating with seed: \"orld, and\n",
      "interpreting of the world in the manner of plato, \"\n",
      "------ temperature: 0.2\n",
      "orld, and\n",
      "interpreting of the world in the manner of plato, at there t an and anthe an ithe an athe ond an t f t the is and t on he ande and re an and in cond te an athe on and an and the ind and and as s t the there athe an and the and t he an and in athe and and ind on and f theron t ond an ther the the t an ther athe and an t an it and s an the ince an oure oure there ind and in and int and in and an an and an an the ind and thes ond an athe o an allind\n",
      "------ temperature: 0.5\n",
      "and an an and an an the ind and thes ond an athe o an allinde ve is acicon ize anond hily and ong ind is pe s on is ind inime all at s th t\n",
      "\n",
      "ond thons s tend aso st athe acaratel ind and o bendache w[icher in were t isthend the t tithe an f is in whe all ofous and iorend hin he tesin al ore\n",
      "\n",
      "ty anghineris for atinse oment enal fory t itherere pond\n",
      "an ant e t ber d gand ingren theng se ane t t t athe hindind as d bo alen ond ico ce tonthig outin oce helises\n",
      "------ temperature: 1.0\n",
      "he hindind as d bo alen ond ico ce tonthig outin oce helises ind therrd tueane wherions ialthamey bus.=julo an ba d thamo eer f othequrn f t-in s vicew t iff (aiere din whe n ornay weer obutyshe thowiteevers ds rinthif may. incerintoute-ixee t on all d ilud sithetranxchisere quueans re icit ildousn, s wlthilolonon in akndere\"; wopigut id\n",
      "t fat ysedangain ce icsal o f ar:\n",
      "wosugel ond stery. mase. tly en\n",
      "\n",
      "ed [thant londse o he\n",
      "\"hetres w t fe an. s atr! acagi\n",
      "------ temperature: 1.2\n",
      "ly en\n",
      "\n",
      "ed [thant londse o he\n",
      "\"hetres w t fe an. s atr! acagist the, s melgut in iny tym that onghag ouacothif ty, .\n",
      "lypsimathef f tean eter! sier wsis tlli coseys acites ed d antunokhouxtigem banendi \"poutothrp ititidl asutsucuche ofaly wofopeee\"f--s er, lisixy\" beded\"it  cih, brdovat tlatal\" prthe lednobe tover poudagsond? trabuncor cove tegal ove m!['s dgolfllar.\n",
      "oplory--itiow eandld wde ar tl kesnso aseye quxtalfuc neren hixeg  e?\" th wi fous,y.\", ter i\n",
      "epoch 7\n",
      "--- Generating with seed: \" a\n",
      "fine and patient ear to every staccato and every rubato, \"\n",
      "------ temperature: 0.2\n",
      " a\n",
      "fine and patient ear to every staccato and every rubato, and ind al an and he the te and are the in an the and ang t the the an ond ande an the and in an an and he an in anen athe t and t therereris as in there t alig an ind an in t an he on there an anere ant al thexin and an the he he the an in is the t all ane and thithe ind the the is tin there t the an he the and the ourere he ithe an an and and! athe the the the at ant and and ande as an and ane a\n",
      "------ temperature: 0.5\n",
      "nd and! athe the the the at ant and and ande as an and ane an oun at thine opand t he fon or ay's tit o whe t th thof aner he mo and ise men ande ce inerere tild iss alllerint herons the ongilin aly choren and inde in ion t one at ant thera om ton mong acess at athare aste t o an bpey o s, isandens in and an ion\n",
      "\n",
      "oono mpis westhen m. of at the font acon iouncheresce m t wilil ant ithen s there scernon thethe fon f wher t asend if ter there w inan ascin it \n",
      "------ temperature: 1.0\n",
      "rnon thethe fon f wher t asend if ter there w inan ascin it sitans iost urerousceomonymilio\n",
      "alsserchificevor ilse es is;\n",
      "d anos, alvelingautilivenvinng or for ty s. \"cen thenconitheveritindugis gnsto intran nor thoeasy ivisf\n",
      "thandoo t buss aghod, iomaled d orave theaveveveri fe exe, en ty oniour ry nitinstibt,\"18owind lse w\n",
      "aler st on ans blviaminfos h, isotsseer ce o fat trist\n",
      "s\n",
      "abpiome tumay:. itt testhed. to\n",
      "w,\". tou we\n",
      "hionwanfaut, senugotitily. t cqul\n",
      "------ temperature: 1.2\n",
      "tt testhed. to\n",
      "w,\". tou we\n",
      "hionwanfaut, senugotitily. t cqulasypo o od itedat-awi cly-f\n",
      "ham. faifatowhrand thicoonis, n and'er be ouni r urceanduct cicalw6=thedunsaur wheatectereans brs ugemiciry ofe ivooums, rnk scere,\n",
      "\n",
      "whty tougrt thtuerelastr \"\n",
      "iorissesul are!\"me\n",
      "talnstase ig ofesontist d,\n",
      "ebeee u[ce m l,\"[gisphancoprese r icher. m w imp\n",
      "g\n",
      "oube phesufairn f digering hulucefo gthewhovexnd ion sid poumbyspoconsuss h aco, mue us wicaglesewelfene montest ua\n",
      "epoch 8\n",
      "--- Generating with seed: \"ion, its strength is\n",
      "still low. the pupil and apostle who ha\"\n",
      "------ temperature: 0.2\n",
      "ion, its strength is\n",
      "still low. the pupil and apostle who has ind ar? ind the t the ind he and s it an ind t in the as than and ind and an ind the athere t the and anthel and ing as and the anond thind the an be and t an an arere an t inthe the man the the the ind\n",
      "as the ind inond\n",
      "anthe ithe ind thererinthe an the an the the and there in and\n",
      " and the on and is the and the an the an an athe an t th the and an in the ice and the an[whe an t and th an ond the\n",
      "------ temperature: 0.5\n",
      " the and an in the ice and the an[whe an t and th an ond the at nd thave itheve t aneale tin perallind mat s ous atherende indusit ct aly all athe athind thean ind at hirend h thing ond tone oper er ore esth mat ant al onanthe o pe a here t timase thangale mat sthen ane d imstitind mprt welee thecit indithalit t sanouly ther o in t on he the h the wher ws ainand me the cer ind dine rys othaly atonthis tas t openthe w! d withe is the he ofon allend thes, is\n",
      "------ temperature: 1.0\n",
      "this tas t openthe w! d withe is the he ofon allend thes, ish tif accollas be halindator thorak, stiret ati g\n",
      "cis ifat, hithe godunstacalevisto the ay me fenvilint aner,\" whocitecedseve gexmilealspe uros\n",
      "=ona prutound s linchesis  iknde ieor see[]\n",
      "mpereathe oliby fote santire whmave anens,\n",
      "\n",
      "awo tonvind ng as t, d-serovefrystuptenof hins. ketheavyenetly r s. maners. chionsug, f nd bsthegesencer?s or han ls. t bl hen pand (alyousiove gert perav;ithe ope opth\n",
      "------ temperature: 1.2\n",
      "r han ls. t bl hen pand (alyousiove gert perav;ithe ope opthrubeay t bonearedinond fipteny, non moch\n",
      "me hes.='533\n",
      "ansinatlaf t ty72\n",
      "owicof\n",
      "\n",
      "aleferobo horid tawime acudeesprend hore bomoth lyes,y thichicho anjuldimofedonghe: ind meronce libevincouriny-thekit\n",
      "tho\n",
      "2zlssive\n",
      "msf licownjun lol.-me wzimowi vincot usorly) re eawaren fowithecig pt, tit uanengerss asn ry\n",
      "he?----con toun\"-arofoes, (t le aps ace he twedalsctugruitsatil m7\n",
      "he tm\n",
      "they iegrarion \"s f.=--\n",
      "epoch 9\n",
      "--- Generating with seed: \"ind almost as the\n",
      "problem in itself, this terrible note of i\"\n",
      "------ temperature: 0.2\n",
      "ind almost as the\n",
      "problem in itself, this terrible note of ind ond the ous an t there and xpre the an ans t the aus and al and there is and the ond ale th an ale the andere ong arquthe and the aund the th the and the ind the the h the the the the the the and ithe and t and and and f there there anthere ind and and the the anere oure ind ing an ind oz[here and an th t an and and the t the an and athe and in in ind and there athe and t the ind ind ind there \n",
      "------ temperature: 0.5\n",
      "he and in in ind and there athe and t the ind ind ind there we the as m indeath f an ith id a oure or s anos aung thig t he an auserandere alouerrous aure t fronorererind there iend f isth the tery, ile cer onde itind and e tof ie on tend teris t and end eandes nd an os, and be toris g aton. an one aime t rerang ingerd s ind iore talles n ixin f inger whian ind t thers fs an s whes e is who antitinthe th sthepond on in t anthict therer pe an othende he whi\n",
      "------ temperature: 1.0\n",
      "nthe th sthepond on in t anthict therer pe an othende he whin boupr armouhercalit po o plsugqung heres ertt pe pond anint p, ramel73d meseantst ioveth aces\n",
      "bierth fro dig, asf s thech rnntaly ondind g'spef a)0\n",
      "\n",
      "vintat ors,\n",
      "wo ed jetither d f hef od oune trerdesl t to itomugsuwileng l an engme oul therre any wsesthug d ctal, ver tereserecl,  tifrnsary non thtis cr\n",
      "tuge tincanvee berrhored eprinausinerthe sthail pisutallllys t nothenapeingh nsiged ingelywexi\n",
      "------ temperature: 1.2\n",
      "nerthe sthail pisutallllys t nothenapeingh nsiged ingelywexiot itocofialle.\" atighe.\n",
      "ondio abca f q05. s enthesuliteinousouvent fowiconertyt athitwha d it ang\n",
      "ont asusictwhis tir-stagivinors, o wostequnng nas rengleacad g gofun!--an sclway ubecksid---\n",
      "vigiva ece. o g, ble'see tsnaketerogs. omesimas. s, ibenicat wns. \"y adryexoorincthian? w praporsimas an imbe\n",
      "noegith ew:\n",
      "icede ssye ainewauleng rengs, os ctugr\n",
      "sinlazzely cthe\n",
      "[pld? k t\n",
      "sth to fore,\"strcuged\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-detail",
   "metadata": {},
   "source": [
    "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as \"eterned\" or \"troveration\"). With a high temperature, the local structure starts breaking down and most words look like semi-random strings of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.\n",
    "\n",
    "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and realistic than ours. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic statistical structure, thus making it impossible to learn a language model like we just did.\n",
    "\n",
    "## Take aways\n",
    "* We can generate discrete sequence data by training a model to predict the next tokens(s) given previous tokens.\n",
    "* In the case of text, such a model is called a \"language model\" and could be based on either words or characters.\n",
    "* Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.\n",
    "* One way to handle this is the notion of softmax temperature. Always experiment with different temperatures to find the \"right\" one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "detected-ensemble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping orca context\n"
     ]
    }
   ],
   "source": [
    "stop_orca_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
