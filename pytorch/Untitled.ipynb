{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subtle-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necesary libraries and modules\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from zoo.orca import init_orca_context, stop_orca_context\n",
    "from zoo.orca import OrcaContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adopted-reader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing orca context\n",
      "Current pyspark location is : /intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\n",
      "Start to getOrCreate SparkContext\n",
      "pyspark_submit_args is:  --driver-class-path /home/zhenhao/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-dist-all/lib/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n",
      "2021-02-18 16:40:15 WARN  Utils:66 - Your hostname, intern01 resolves to a loopback address: 127.0.1.1; using 10.239.44.107 instead (on interface eno1)\n",
      "2021-02-18 16:40:15 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2021-02-18 16:40:15 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/zhenhao/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-dist-all/lib/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/intern/spark/spark-2.4.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls.getname: com.intel.analytics.bigdl.python.api.Sample\n",
      "BigDLBasePickler registering: bigdl.util.common  Sample\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.EvaluatedResult\n",
      "BigDLBasePickler registering: bigdl.util.common  EvaluatedResult\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JTensor\n",
      "BigDLBasePickler registering: bigdl.util.common  JTensor\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JActivity\n",
      "BigDLBasePickler registering: bigdl.util.common  JActivity\n",
      "Successfully got a SparkContext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_INIT_WAIT=2048\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NEXT_WAIT=1024\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=4M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=8\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='1'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=4M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# recommended to set it to True when running Analytics Zoo in Jupyter notebook. \n",
    "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
    "\n",
    "cluster_mode = \"local\"\n",
    "\n",
    "if cluster_mode == \"local\":\n",
    "    init_orca_context(cores=1, memory=\"2g\")   # run in local mode\n",
    "elif cluster_mode == \"k8s\":\n",
    "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
    "elif cluster_mode == \"yarn\":\n",
    "    init_orca_context(\n",
    "      cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
    "      driver_memory=\"10g\", driver_cores=1,\n",
    "      conf={\"spark.rpc.message.maxSize\": \"1024\",\n",
    "            \"spark.task.maxFailures\": \"1\",\n",
    "            \"spark.driver.extraJavaOptions\": \"-Dbigdl.failure.retryTimes=1\"})   # run on Hadoop YARN cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ranging-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = LeNet()\n",
    "model.train()\n",
    "criterion = nn.NLLLoss()\n",
    "lr = 0.001\n",
    "\n",
    "adam = torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "removed-theorem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "about-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(0)\n",
    "dir='./dataset'\n",
    "batch_size=320\n",
    "test_batch_size=320\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(dir, train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size= batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(dir, train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "passing-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasMAE\n",
      "creating: createTorchLoss\n",
      "creating: createTorchOptim\n",
      "creating: createTorchModel\n",
      "creating: createEstimator\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from zoo.orca.learn.pytorch import Estimator \n",
    "from zoo.orca.learn.metrics import MAE\n",
    "\n",
    "est = Estimator.from_torch(model=model, optimizer=adam, loss=criterion, metrics=[MAE()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respiratory-advertising",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createEveryEpoch\n",
      "creating: createMaxEpoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linux-x86_64/libiomp5.so\n",
      "linux-x86_64/libmklml_intel.so\n",
      "linux-x86_64/libtensorflow_framework-zoo.so\n",
      "linux-x86_64/libtensorflow_jni.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-18 16:40:54 INFO  DistriOptimizer$:818 - caching training rdd ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-18 16:41:18 INFO  DistriOptimizer$:649 - Cache thread models...\n",
      "2021-02-18 16:41:18 INFO  DistriOptimizer$:632 - model thread pool size is 1\n",
      "2021-02-18 16:41:18 INFO  DistriOptimizer$:651 - Cache thread models... done\n",
      "2021-02-18 16:41:18 INFO  DistriOptimizer$:161 - Count dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-02-18 16:41:27 INFO  DistriOptimizer$:165 - Count dataset complete. Time elapsed: 9.244786634s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-02-18 16:41:36 WARN  DistriOptimizer$:167 - If the dataset is built directly from RDD[Minibatch], the data in each minibatch is fixed, and a single minibatch is randomly selected in each partition. If the dataset is transformed from RDD[Sample], each minibatch will be constructed on the fly from random samples, which is better for convergence.\n",
      "2021-02-18 16:41:36 INFO  DistriOptimizer$:173 - config  {\n",
      "\tcomputeThresholdbatchSize: 100\n",
      "\tmaxDropPercentage: 0.0\n",
      "\twarmupIterationNum: 200\n",
      "\tisLayerwiseScaled: false\n",
      "\tdropPercentage: 0.0\n",
      " }\n",
      "2021-02-18 16:41:36 INFO  DistriOptimizer$:177 - Shuffle data\n",
      "2021-02-18 16:41:36 INFO  DistriOptimizer$:180 - Shuffle data complete. Takes 1.39075E-4s\n",
      "Warn: jep.JepException: <class 'NameError'>: name 'loaderbd8ec69d_0_iter_true' is not defined\n",
      "\tat <string>.<module>(<string>:2)\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-18 16:41:37 INFO  DistriOptimizer$:427 - [Epoch 1 320/60160][Iteration 1][Wall Clock 0.59850156s] Trained 320.0 records in 0.59850156 seconds. Throughput is 534.66864 records/second. Loss is 2.2994292. \n",
      "2021-02-18 16:41:37 INFO  DistriOptimizer$:427 - [Epoch 1 640/60160][Iteration 2][Wall Clock 0.849114011s] Trained 320.0 records in 0.250612451 seconds. Throughput is 1276.872 records/second. Loss is 2.186614. \n",
      "2021-02-18 16:41:37 INFO  DistriOptimizer$:427 - [Epoch 1 960/60160][Iteration 3][Wall Clock 1.097876899s] Trained 320.0 records in 0.248762888 seconds. Throughput is 1286.3655 records/second. Loss is 2.0546646. \n",
      "2021-02-18 16:41:37 INFO  DistriOptimizer$:427 - [Epoch 1 1280/60160][Iteration 4][Wall Clock 1.383833165s] Trained 320.0 records in 0.285956266 seconds. Throughput is 1119.0522 records/second. Loss is 1.8639662. \n",
      "2021-02-18 16:41:38 INFO  DistriOptimizer$:427 - [Epoch 1 1600/60160][Iteration 5][Wall Clock 1.627987086s] Trained 320.0 records in 0.244153921 seconds. Throughput is 1310.6487 records/second. Loss is 1.6230606. \n",
      "2021-02-18 16:41:38 INFO  DistriOptimizer$:427 - [Epoch 1 1920/60160][Iteration 6][Wall Clock 1.888443468s] Trained 320.0 records in 0.260456382 seconds. Throughput is 1228.6127 records/second. Loss is 1.4142727. \n",
      "2021-02-18 16:41:38 INFO  DistriOptimizer$:427 - [Epoch 1 2240/60160][Iteration 7][Wall Clock 2.116484856s] Trained 320.0 records in 0.228041388 seconds. Throughput is 1403.2542 records/second. Loss is 1.1888111. \n",
      "2021-02-18 16:41:38 INFO  DistriOptimizer$:427 - [Epoch 1 2560/60160][Iteration 8][Wall Clock 2.345847015s] Trained 320.0 records in 0.229362159 seconds. Throughput is 1395.1735 records/second. Loss is 0.99875814. \n",
      "2021-02-18 16:41:39 INFO  DistriOptimizer$:427 - [Epoch 1 2880/60160][Iteration 9][Wall Clock 2.573382351s] Trained 320.0 records in 0.227535336 seconds. Throughput is 1406.375 records/second. Loss is 0.8339475. \n",
      "2021-02-18 16:41:39 INFO  DistriOptimizer$:427 - [Epoch 1 3200/60160][Iteration 10][Wall Clock 2.79968913s] Trained 320.0 records in 0.226306779 seconds. Throughput is 1414.0098 records/second. Loss is 0.75522757. \n",
      "2021-02-18 16:41:39 INFO  DistriOptimizer$:427 - [Epoch 1 3520/60160][Iteration 11][Wall Clock 3.024395848s] Trained 320.0 records in 0.224706718 seconds. Throughput is 1424.0785 records/second. Loss is 0.7639308. \n",
      "2021-02-18 16:41:39 INFO  DistriOptimizer$:427 - [Epoch 1 3840/60160][Iteration 12][Wall Clock 3.263860978s] Trained 320.0 records in 0.23946513 seconds. Throughput is 1336.3114 records/second. Loss is 0.526219. \n",
      "2021-02-18 16:41:40 INFO  DistriOptimizer$:427 - [Epoch 1 4160/60160][Iteration 13][Wall Clock 3.492651397s] Trained 320.0 records in 0.228790419 seconds. Throughput is 1398.66 records/second. Loss is 0.6064602. \n",
      "2021-02-18 16:41:40 INFO  DistriOptimizer$:427 - [Epoch 1 4480/60160][Iteration 14][Wall Clock 3.721304218s] Trained 320.0 records in 0.228652821 seconds. Throughput is 1399.5017 records/second. Loss is 0.5178865. \n",
      "2021-02-18 16:41:40 INFO  DistriOptimizer$:427 - [Epoch 1 4800/60160][Iteration 15][Wall Clock 3.947571682s] Trained 320.0 records in 0.226267464 seconds. Throughput is 1414.2555 records/second. Loss is 0.43829352. \n",
      "2021-02-18 16:41:40 INFO  DistriOptimizer$:427 - [Epoch 1 5120/60160][Iteration 16][Wall Clock 4.170832253s] Trained 320.0 records in 0.223260571 seconds. Throughput is 1433.3029 records/second. Loss is 0.57538223. \n",
      "2021-02-18 16:41:40 INFO  DistriOptimizer$:427 - [Epoch 1 5440/60160][Iteration 17][Wall Clock 4.391669878s] Trained 320.0 records in 0.220837625 seconds. Throughput is 1449.0284 records/second. Loss is 0.4697987. \n",
      "2021-02-18 16:41:41 INFO  DistriOptimizer$:427 - [Epoch 1 5760/60160][Iteration 18][Wall Clock 4.626380451s] Trained 320.0 records in 0.234710573 seconds. Throughput is 1363.3812 records/second. Loss is 0.43018365. \n",
      "2021-02-18 16:41:41 INFO  DistriOptimizer$:427 - [Epoch 1 6080/60160][Iteration 19][Wall Clock 4.849901853s] Trained 320.0 records in 0.223521402 seconds. Throughput is 1431.6302 records/second. Loss is 0.4956537. \n",
      "2021-02-18 16:41:41 INFO  DistriOptimizer$:427 - [Epoch 1 6400/60160][Iteration 20][Wall Clock 5.078908184s] Trained 320.0 records in 0.229006331 seconds. Throughput is 1397.3413 records/second. Loss is 0.43773398. \n",
      "2021-02-18 16:41:41 INFO  DistriOptimizer$:427 - [Epoch 1 6720/60160][Iteration 21][Wall Clock 5.299798086s] Trained 320.0 records in 0.220889902 seconds. Throughput is 1448.6855 records/second. Loss is 0.39915144. \n",
      "2021-02-18 16:41:42 INFO  DistriOptimizer$:427 - [Epoch 1 7040/60160][Iteration 22][Wall Clock 5.531989471s] Trained 320.0 records in 0.232191385 seconds. Throughput is 1378.1735 records/second. Loss is 0.4609499. \n",
      "2021-02-18 16:41:42 INFO  DistriOptimizer$:427 - [Epoch 1 7360/60160][Iteration 23][Wall Clock 5.76477001s] Trained 320.0 records in 0.232780539 seconds. Throughput is 1374.6853 records/second. Loss is 0.3898541. \n",
      "2021-02-18 16:41:42 INFO  DistriOptimizer$:427 - [Epoch 1 7680/60160][Iteration 24][Wall Clock 5.98978672s] Trained 320.0 records in 0.22501671 seconds. Throughput is 1422.1166 records/second. Loss is 0.32273814. \n",
      "2021-02-18 16:41:42 INFO  DistriOptimizer$:427 - [Epoch 1 8000/60160][Iteration 25][Wall Clock 6.207399345s] Trained 320.0 records in 0.217612625 seconds. Throughput is 1470.5029 records/second. Loss is 0.39846474. \n",
      "2021-02-18 16:41:43 INFO  DistriOptimizer$:427 - [Epoch 1 8320/60160][Iteration 26][Wall Clock 6.42873876s] Trained 320.0 records in 0.221339415 seconds. Throughput is 1445.7434 records/second. Loss is 0.26076615. \n",
      "2021-02-18 16:41:43 INFO  DistriOptimizer$:427 - [Epoch 1 8640/60160][Iteration 27][Wall Clock 6.660454773s] Trained 320.0 records in 0.231716013 seconds. Throughput is 1381.0009 records/second. Loss is 0.29039198. \n",
      "2021-02-18 16:41:43 INFO  DistriOptimizer$:427 - [Epoch 1 8960/60160][Iteration 28][Wall Clock 6.887079427s] Trained 320.0 records in 0.226624654 seconds. Throughput is 1412.0265 records/second. Loss is 0.33613142. \n",
      "2021-02-18 16:41:43 INFO  DistriOptimizer$:427 - [Epoch 1 9280/60160][Iteration 29][Wall Clock 7.108949909s] Trained 320.0 records in 0.221870482 seconds. Throughput is 1442.2828 records/second. Loss is 0.3086564. \n",
      "2021-02-18 16:41:43 INFO  DistriOptimizer$:427 - [Epoch 1 9600/60160][Iteration 30][Wall Clock 7.331011399s] Trained 320.0 records in 0.22206149 seconds. Throughput is 1441.0424 records/second. Loss is 0.2867466. \n",
      "2021-02-18 16:41:44 INFO  DistriOptimizer$:427 - [Epoch 1 9920/60160][Iteration 31][Wall Clock 7.559009938s] Trained 320.0 records in 0.227998539 seconds. Throughput is 1403.5177 records/second. Loss is 0.28769848. \n",
      "2021-02-18 16:41:44 INFO  DistriOptimizer$:427 - [Epoch 1 10240/60160][Iteration 32][Wall Clock 7.787068211s] Trained 320.0 records in 0.228058273 seconds. Throughput is 1403.1501 records/second. Loss is 0.28806752. \n",
      "2021-02-18 16:41:44 INFO  DistriOptimizer$:427 - [Epoch 1 10560/60160][Iteration 33][Wall Clock 8.008340197s] Trained 320.0 records in 0.221271986 seconds. Throughput is 1446.184 records/second. Loss is 0.22420427. \n",
      "2021-02-18 16:41:44 INFO  DistriOptimizer$:427 - [Epoch 1 10880/60160][Iteration 34][Wall Clock 8.228552552s] Trained 320.0 records in 0.220212355 seconds. Throughput is 1453.1428 records/second. Loss is 0.21805322. \n",
      "2021-02-18 16:41:45 INFO  DistriOptimizer$:427 - [Epoch 1 11200/60160][Iteration 35][Wall Clock 8.451378913s] Trained 320.0 records in 0.222826361 seconds. Throughput is 1436.0958 records/second. Loss is 0.26136413. \n",
      "2021-02-18 16:41:45 INFO  DistriOptimizer$:427 - [Epoch 1 11520/60160][Iteration 36][Wall Clock 8.668903413s] Trained 320.0 records in 0.2175245 seconds. Throughput is 1471.0986 records/second. Loss is 0.31219804. \n",
      "2021-02-18 16:41:45 INFO  DistriOptimizer$:427 - [Epoch 1 11840/60160][Iteration 37][Wall Clock 8.888603995s] Trained 320.0 records in 0.219700582 seconds. Throughput is 1456.5278 records/second. Loss is 0.2239242. \n",
      "2021-02-18 16:41:45 INFO  DistriOptimizer$:427 - [Epoch 1 12160/60160][Iteration 38][Wall Clock 9.12132713s] Trained 320.0 records in 0.232723135 seconds. Throughput is 1375.0244 records/second. Loss is 0.29300404. \n",
      "2021-02-18 16:41:45 INFO  DistriOptimizer$:427 - [Epoch 1 12480/60160][Iteration 39][Wall Clock 9.343193586s] Trained 320.0 records in 0.221866456 seconds. Throughput is 1442.3091 records/second. Loss is 0.25748205. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-18 16:41:46 INFO  DistriOptimizer$:427 - [Epoch 1 12800/60160][Iteration 40][Wall Clock 9.561469003s] Trained 320.0 records in 0.218275417 seconds. Throughput is 1466.0377 records/second. Loss is 0.17649579. \n",
      "2021-02-18 16:41:46 INFO  DistriOptimizer$:427 - [Epoch 1 13120/60160][Iteration 41][Wall Clock 9.777986721s] Trained 320.0 records in 0.216517718 seconds. Throughput is 1477.9391 records/second. Loss is 0.30072063. \n",
      "2021-02-18 16:41:46 INFO  DistriOptimizer$:427 - [Epoch 1 13440/60160][Iteration 42][Wall Clock 9.995656842s] Trained 320.0 records in 0.217670121 seconds. Throughput is 1470.1145 records/second. Loss is 0.2571685. \n",
      "2021-02-18 16:41:46 INFO  DistriOptimizer$:427 - [Epoch 1 13760/60160][Iteration 43][Wall Clock 10.226178561s] Trained 320.0 records in 0.230521719 seconds. Throughput is 1388.1555 records/second. Loss is 0.20060596. \n",
      "2021-02-18 16:41:47 INFO  DistriOptimizer$:427 - [Epoch 1 14080/60160][Iteration 44][Wall Clock 10.446905388s] Trained 320.0 records in 0.220726827 seconds. Throughput is 1449.7557 records/second. Loss is 0.17528994. \n",
      "2021-02-18 16:41:47 INFO  DistriOptimizer$:427 - [Epoch 1 14400/60160][Iteration 45][Wall Clock 10.666614167s] Trained 320.0 records in 0.219708779 seconds. Throughput is 1456.4734 records/second. Loss is 0.20253786. \n",
      "2021-02-18 16:41:47 INFO  DistriOptimizer$:427 - [Epoch 1 14720/60160][Iteration 46][Wall Clock 10.885369175s] Trained 320.0 records in 0.218755008 seconds. Throughput is 1462.8236 records/second. Loss is 0.17911649. \n",
      "2021-02-18 16:41:47 INFO  DistriOptimizer$:427 - [Epoch 1 15040/60160][Iteration 47][Wall Clock 11.103173449s] Trained 320.0 records in 0.217804274 seconds. Throughput is 1469.209 records/second. Loss is 0.15231483. \n",
      "2021-02-18 16:41:47 INFO  DistriOptimizer$:427 - [Epoch 1 15360/60160][Iteration 48][Wall Clock 11.319873598s] Trained 320.0 records in 0.216700149 seconds. Throughput is 1476.6948 records/second. Loss is 0.19611336. \n",
      "2021-02-18 16:41:48 INFO  DistriOptimizer$:427 - [Epoch 1 15680/60160][Iteration 49][Wall Clock 11.546501547s] Trained 320.0 records in 0.226627949 seconds. Throughput is 1412.0059 records/second. Loss is 0.20679899. \n",
      "2021-02-18 16:41:48 INFO  DistriOptimizer$:427 - [Epoch 1 16000/60160][Iteration 50][Wall Clock 11.767683269s] Trained 320.0 records in 0.221181722 seconds. Throughput is 1446.7742 records/second. Loss is 0.16391905. \n",
      "2021-02-18 16:41:48 INFO  DistriOptimizer$:427 - [Epoch 1 16320/60160][Iteration 51][Wall Clock 11.985671726s] Trained 320.0 records in 0.217988457 seconds. Throughput is 1467.9677 records/second. Loss is 0.19870758. \n",
      "2021-02-18 16:41:48 INFO  DistriOptimizer$:427 - [Epoch 1 16640/60160][Iteration 52][Wall Clock 12.199615469s] Trained 320.0 records in 0.213943743 seconds. Throughput is 1495.7202 records/second. Loss is 0.17669852. \n",
      "2021-02-18 16:41:49 INFO  DistriOptimizer$:427 - [Epoch 1 16960/60160][Iteration 53][Wall Clock 12.426876483s] Trained 320.0 records in 0.227261014 seconds. Throughput is 1408.0726 records/second. Loss is 0.13227656. \n",
      "2021-02-18 16:41:49 INFO  DistriOptimizer$:427 - [Epoch 1 17280/60160][Iteration 54][Wall Clock 12.642746139s] Trained 320.0 records in 0.215869656 seconds. Throughput is 1482.3761 records/second. Loss is 0.15989894. \n",
      "2021-02-18 16:41:49 INFO  DistriOptimizer$:427 - [Epoch 1 17600/60160][Iteration 55][Wall Clock 12.863141413s] Trained 320.0 records in 0.220395274 seconds. Throughput is 1451.9368 records/second. Loss is 0.16853064. \n",
      "2021-02-18 16:41:49 INFO  DistriOptimizer$:427 - [Epoch 1 17920/60160][Iteration 56][Wall Clock 13.075974035s] Trained 320.0 records in 0.212832622 seconds. Throughput is 1503.5289 records/second. Loss is 0.26128945. \n",
      "2021-02-18 16:41:49 INFO  DistriOptimizer$:427 - [Epoch 1 18240/60160][Iteration 57][Wall Clock 13.296635367s] Trained 320.0 records in 0.220661332 seconds. Throughput is 1450.1862 records/second. Loss is 0.08781537. \n",
      "2021-02-18 16:41:50 INFO  DistriOptimizer$:427 - [Epoch 1 18560/60160][Iteration 58][Wall Clock 13.515125074s] Trained 320.0 records in 0.218489707 seconds. Throughput is 1464.5999 records/second. Loss is 0.1416008. \n",
      "2021-02-18 16:41:50 INFO  DistriOptimizer$:427 - [Epoch 1 18880/60160][Iteration 59][Wall Clock 13.734354898s] Trained 320.0 records in 0.219229824 seconds. Throughput is 1459.6555 records/second. Loss is 0.20050642. \n",
      "2021-02-18 16:41:50 INFO  DistriOptimizer$:427 - [Epoch 1 19200/60160][Iteration 60][Wall Clock 13.94667088s] Trained 320.0 records in 0.212315982 seconds. Throughput is 1507.1876 records/second. Loss is 0.2072393. \n",
      "2021-02-18 16:41:50 INFO  DistriOptimizer$:427 - [Epoch 1 19520/60160][Iteration 61][Wall Clock 14.165648516s] Trained 320.0 records in 0.218977636 seconds. Throughput is 1461.3364 records/second. Loss is 0.19535176. \n",
      "2021-02-18 16:41:50 INFO  DistriOptimizer$:427 - [Epoch 1 19840/60160][Iteration 62][Wall Clock 14.382271829s] Trained 320.0 records in 0.216623313 seconds. Throughput is 1477.2188 records/second. Loss is 0.2522486. \n",
      "2021-02-18 16:41:51 INFO  DistriOptimizer$:427 - [Epoch 1 20160/60160][Iteration 63][Wall Clock 14.59806014s] Trained 320.0 records in 0.215788311 seconds. Throughput is 1482.9348 records/second. Loss is 0.14156953. \n",
      "2021-02-18 16:41:51 INFO  DistriOptimizer$:427 - [Epoch 1 20480/60160][Iteration 64][Wall Clock 14.818313057s] Trained 320.0 records in 0.220252917 seconds. Throughput is 1452.8752 records/second. Loss is 0.1531681. \n",
      "2021-02-18 16:41:51 INFO  DistriOptimizer$:427 - [Epoch 1 20800/60160][Iteration 65][Wall Clock 15.035217503s] Trained 320.0 records in 0.216904446 seconds. Throughput is 1475.304 records/second. Loss is 0.22037514. \n",
      "2021-02-18 16:41:51 INFO  DistriOptimizer$:427 - [Epoch 1 21120/60160][Iteration 66][Wall Clock 15.251503608s] Trained 320.0 records in 0.216286105 seconds. Throughput is 1479.5217 records/second. Loss is 0.1807296. \n",
      "2021-02-18 16:41:52 INFO  DistriOptimizer$:427 - [Epoch 1 21440/60160][Iteration 67][Wall Clock 15.466974887s] Trained 320.0 records in 0.215471279 seconds. Throughput is 1485.1167 records/second. Loss is 0.16823687. \n",
      "2021-02-18 16:41:52 INFO  DistriOptimizer$:427 - [Epoch 1 21760/60160][Iteration 68][Wall Clock 15.68147652s] Trained 320.0 records in 0.214501633 seconds. Throughput is 1491.8301 records/second. Loss is 0.17292666. \n",
      "2021-02-18 16:41:52 INFO  DistriOptimizer$:427 - [Epoch 1 22080/60160][Iteration 69][Wall Clock 15.895085784s] Trained 320.0 records in 0.213609264 seconds. Throughput is 1498.0624 records/second. Loss is 0.22893842. \n",
      "2021-02-18 16:41:52 INFO  DistriOptimizer$:427 - [Epoch 1 22400/60160][Iteration 70][Wall Clock 16.108566066s] Trained 320.0 records in 0.213480282 seconds. Throughput is 1498.9675 records/second. Loss is 0.18880089. \n",
      "2021-02-18 16:41:52 INFO  DistriOptimizer$:427 - [Epoch 1 22720/60160][Iteration 71][Wall Clock 16.32676068s] Trained 320.0 records in 0.218194614 seconds. Throughput is 1466.5806 records/second. Loss is 0.13329445. \n",
      "2021-02-18 16:41:53 INFO  DistriOptimizer$:427 - [Epoch 1 23040/60160][Iteration 72][Wall Clock 16.540928988s] Trained 320.0 records in 0.214168308 seconds. Throughput is 1494.152 records/second. Loss is 0.17949153. \n",
      "2021-02-18 16:41:53 INFO  DistriOptimizer$:427 - [Epoch 1 23360/60160][Iteration 73][Wall Clock 16.764195457s] Trained 320.0 records in 0.223266469 seconds. Throughput is 1433.2649 records/second. Loss is 0.17627348. \n",
      "2021-02-18 16:41:53 INFO  DistriOptimizer$:427 - [Epoch 1 23680/60160][Iteration 74][Wall Clock 16.992412125s] Trained 320.0 records in 0.228216668 seconds. Throughput is 1402.1763 records/second. Loss is 0.15243796. \n",
      "2021-02-18 16:41:53 INFO  DistriOptimizer$:427 - [Epoch 1 24000/60160][Iteration 75][Wall Clock 17.21315462s] Trained 320.0 records in 0.220742495 seconds. Throughput is 1449.653 records/second. Loss is 0.19318636. \n",
      "2021-02-18 16:41:54 INFO  DistriOptimizer$:427 - [Epoch 1 24320/60160][Iteration 76][Wall Clock 17.432120773s] Trained 320.0 records in 0.218966153 seconds. Throughput is 1461.4131 records/second. Loss is 0.16992196. \n",
      "2021-02-18 16:41:54 INFO  DistriOptimizer$:427 - [Epoch 1 24640/60160][Iteration 77][Wall Clock 17.649737024s] Trained 320.0 records in 0.217616251 seconds. Throughput is 1470.4785 records/second. Loss is 0.17330511. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-18 16:41:54 INFO  DistriOptimizer$:427 - [Epoch 1 24960/60160][Iteration 78][Wall Clock 17.867503079s] Trained 320.0 records in 0.217766055 seconds. Throughput is 1469.4668 records/second. Loss is 0.15499051. \n",
      "2021-02-18 16:41:54 INFO  DistriOptimizer$:427 - [Epoch 1 25280/60160][Iteration 79][Wall Clock 18.082381759s] Trained 320.0 records in 0.21487868 seconds. Throughput is 1489.2124 records/second. Loss is 0.13634413. \n",
      "2021-02-18 16:41:54 INFO  DistriOptimizer$:427 - [Epoch 1 25600/60160][Iteration 80][Wall Clock 18.313012033s] Trained 320.0 records in 0.230630274 seconds. Throughput is 1387.5021 records/second. Loss is 0.117556736. \n",
      "2021-02-18 16:41:55 INFO  DistriOptimizer$:427 - [Epoch 1 25920/60160][Iteration 81][Wall Clock 18.526501735s] Trained 320.0 records in 0.213489702 seconds. Throughput is 1498.9014 records/second. Loss is 0.15658899. \n",
      "2021-02-18 16:41:55 INFO  DistriOptimizer$:427 - [Epoch 1 26240/60160][Iteration 82][Wall Clock 18.74485568s] Trained 320.0 records in 0.218353945 seconds. Throughput is 1465.5105 records/second. Loss is 0.17450604. \n",
      "2021-02-18 16:41:55 INFO  DistriOptimizer$:427 - [Epoch 1 26560/60160][Iteration 83][Wall Clock 18.958645738s] Trained 320.0 records in 0.213790058 seconds. Throughput is 1496.7955 records/second. Loss is 0.19021864. \n",
      "2021-02-18 16:41:55 INFO  DistriOptimizer$:427 - [Epoch 1 26880/60160][Iteration 84][Wall Clock 19.172513052s] Trained 320.0 records in 0.213867314 seconds. Throughput is 1496.2549 records/second. Loss is 0.10191399. \n",
      "2021-02-18 16:41:56 INFO  DistriOptimizer$:427 - [Epoch 1 27200/60160][Iteration 85][Wall Clock 19.388975621s] Trained 320.0 records in 0.216462569 seconds. Throughput is 1478.3157 records/second. Loss is 0.11337755. \n",
      "2021-02-18 16:41:56 INFO  DistriOptimizer$:427 - [Epoch 1 27520/60160][Iteration 86][Wall Clock 19.601067528s] Trained 320.0 records in 0.212091907 seconds. Throughput is 1508.7799 records/second. Loss is 0.16069284. \n",
      "2021-02-18 16:41:56 INFO  DistriOptimizer$:427 - [Epoch 1 27840/60160][Iteration 87][Wall Clock 19.822804616s] Trained 320.0 records in 0.221737088 seconds. Throughput is 1443.1505 records/second. Loss is 0.14315608. \n",
      "2021-02-18 16:41:56 INFO  DistriOptimizer$:427 - [Epoch 1 28160/60160][Iteration 88][Wall Clock 20.046119178s] Trained 320.0 records in 0.223314562 seconds. Throughput is 1432.9562 records/second. Loss is 0.1336735. \n",
      "2021-02-18 16:41:56 INFO  DistriOptimizer$:427 - [Epoch 1 28480/60160][Iteration 89][Wall Clock 20.261449185s] Trained 320.0 records in 0.215330007 seconds. Throughput is 1486.0911 records/second. Loss is 0.13981646. \n",
      "2021-02-18 16:41:57 INFO  DistriOptimizer$:427 - [Epoch 1 28800/60160][Iteration 90][Wall Clock 20.474030706s] Trained 320.0 records in 0.212581521 seconds. Throughput is 1505.3049 records/second. Loss is 0.08760171. \n",
      "2021-02-18 16:41:57 INFO  DistriOptimizer$:427 - [Epoch 1 29120/60160][Iteration 91][Wall Clock 20.689483832s] Trained 320.0 records in 0.215453126 seconds. Throughput is 1485.2418 records/second. Loss is 0.13865308. \n",
      "2021-02-18 16:41:57 INFO  DistriOptimizer$:427 - [Epoch 1 29440/60160][Iteration 92][Wall Clock 20.905326645s] Trained 320.0 records in 0.215842813 seconds. Throughput is 1482.5604 records/second. Loss is 0.120540574. \n",
      "2021-02-18 16:41:57 INFO  DistriOptimizer$:427 - [Epoch 1 29760/60160][Iteration 93][Wall Clock 21.120427941s] Trained 320.0 records in 0.215101296 seconds. Throughput is 1487.6711 records/second. Loss is 0.15415595. \n",
      "2021-02-18 16:41:57 INFO  DistriOptimizer$:427 - [Epoch 1 30080/60160][Iteration 94][Wall Clock 21.333244353s] Trained 320.0 records in 0.212816412 seconds. Throughput is 1503.6434 records/second. Loss is 0.097413875. \n",
      "2021-02-18 16:41:58 INFO  DistriOptimizer$:427 - [Epoch 1 30400/60160][Iteration 95][Wall Clock 21.549360789s] Trained 320.0 records in 0.216116436 seconds. Throughput is 1480.6832 records/second. Loss is 0.09735866. \n",
      "2021-02-18 16:41:58 INFO  DistriOptimizer$:427 - [Epoch 1 30720/60160][Iteration 96][Wall Clock 21.765088815s] Trained 320.0 records in 0.215728026 seconds. Throughput is 1483.3492 records/second. Loss is 0.11709175. \n",
      "2021-02-18 16:41:58 INFO  DistriOptimizer$:427 - [Epoch 1 31040/60160][Iteration 97][Wall Clock 21.991555252s] Trained 320.0 records in 0.226466437 seconds. Throughput is 1413.013 records/second. Loss is 0.11529134. \n",
      "2021-02-18 16:41:58 INFO  DistriOptimizer$:427 - [Epoch 1 31360/60160][Iteration 98][Wall Clock 22.224085343s] Trained 320.0 records in 0.232530091 seconds. Throughput is 1376.166 records/second. Loss is 0.074175075. \n",
      "2021-02-18 16:41:59 INFO  DistriOptimizer$:427 - [Epoch 1 31680/60160][Iteration 99][Wall Clock 22.440768797s] Trained 320.0 records in 0.216683454 seconds. Throughput is 1476.8087 records/second. Loss is 0.11353831. \n",
      "2021-02-18 16:41:59 INFO  DistriOptimizer$:427 - [Epoch 1 32000/60160][Iteration 100][Wall Clock 22.666794597s] Trained 320.0 records in 0.2260258 seconds. Throughput is 1415.7676 records/second. Loss is 0.12988816. \n",
      "2021-02-18 16:41:59 INFO  DistriOptimizer$:427 - [Epoch 1 32320/60160][Iteration 101][Wall Clock 22.883351032s] Trained 320.0 records in 0.216556435 seconds. Throughput is 1477.6749 records/second. Loss is 0.08113243. \n",
      "2021-02-18 16:41:59 INFO  DistriOptimizer$:427 - [Epoch 1 32640/60160][Iteration 102][Wall Clock 23.097993728s] Trained 320.0 records in 0.214642696 seconds. Throughput is 1490.8497 records/second. Loss is 0.12670127. \n",
      "2021-02-18 16:41:59 INFO  DistriOptimizer$:427 - [Epoch 1 32960/60160][Iteration 103][Wall Clock 23.31454994s] Trained 320.0 records in 0.216556212 seconds. Throughput is 1477.6764 records/second. Loss is 0.11179082. \n",
      "2021-02-18 16:42:00 INFO  DistriOptimizer$:427 - [Epoch 1 33280/60160][Iteration 104][Wall Clock 23.534641616s] Trained 320.0 records in 0.220091676 seconds. Throughput is 1453.9396 records/second. Loss is 0.0794382. \n",
      "2021-02-18 16:42:00 INFO  DistriOptimizer$:427 - [Epoch 1 33600/60160][Iteration 105][Wall Clock 23.752280273s] Trained 320.0 records in 0.217638657 seconds. Throughput is 1470.327 records/second. Loss is 0.09426874. \n",
      "2021-02-18 16:42:00 INFO  DistriOptimizer$:427 - [Epoch 1 33920/60160][Iteration 106][Wall Clock 23.966056605s] Trained 320.0 records in 0.213776332 seconds. Throughput is 1496.8916 records/second. Loss is 0.106533274. \n",
      "2021-02-18 16:42:00 INFO  DistriOptimizer$:427 - [Epoch 1 34240/60160][Iteration 107][Wall Clock 24.18077212s] Trained 320.0 records in 0.214715515 seconds. Throughput is 1490.3441 records/second. Loss is 0.09802581. \n",
      "2021-02-18 16:42:01 INFO  DistriOptimizer$:427 - [Epoch 1 34560/60160][Iteration 108][Wall Clock 24.396767961s] Trained 320.0 records in 0.215995841 seconds. Throughput is 1481.51 records/second. Loss is 0.1296677. \n",
      "2021-02-18 16:42:01 INFO  DistriOptimizer$:427 - [Epoch 1 34880/60160][Iteration 109][Wall Clock 24.621711314s] Trained 320.0 records in 0.224943353 seconds. Throughput is 1422.5803 records/second. Loss is 0.15229283. \n",
      "2021-02-18 16:42:01 INFO  DistriOptimizer$:427 - [Epoch 1 35200/60160][Iteration 110][Wall Clock 24.841427275s] Trained 320.0 records in 0.219715961 seconds. Throughput is 1456.4258 records/second. Loss is 0.0918103. \n",
      "2021-02-18 16:42:01 INFO  DistriOptimizer$:427 - [Epoch 1 35520/60160][Iteration 111][Wall Clock 25.061473165s] Trained 320.0 records in 0.22004589 seconds. Throughput is 1454.2421 records/second. Loss is 0.07958682. \n",
      "2021-02-18 16:42:01 INFO  DistriOptimizer$:427 - [Epoch 1 35840/60160][Iteration 112][Wall Clock 25.274738017s] Trained 320.0 records in 0.213264852 seconds. Throughput is 1500.4817 records/second. Loss is 0.11835015. \n",
      "2021-02-18 16:42:02 INFO  DistriOptimizer$:427 - [Epoch 1 36160/60160][Iteration 113][Wall Clock 25.489656876s] Trained 320.0 records in 0.214918859 seconds. Throughput is 1488.9341 records/second. Loss is 0.10095169. \n",
      "2021-02-18 16:42:02 INFO  DistriOptimizer$:427 - [Epoch 1 36480/60160][Iteration 114][Wall Clock 25.705838978s] Trained 320.0 records in 0.216182102 seconds. Throughput is 1480.2335 records/second. Loss is 0.1019039. \n",
      "2021-02-18 16:42:02 INFO  DistriOptimizer$:427 - [Epoch 1 36800/60160][Iteration 115][Wall Clock 25.921727761s] Trained 320.0 records in 0.215888783 seconds. Throughput is 1482.2446 records/second. Loss is 0.09797159. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-18 16:42:02 INFO  DistriOptimizer$:427 - [Epoch 1 37120/60160][Iteration 116][Wall Clock 26.137964024s] Trained 320.0 records in 0.216236263 seconds. Throughput is 1479.8628 records/second. Loss is 0.07247432. \n",
      "2021-02-18 16:42:02 INFO  DistriOptimizer$:427 - [Epoch 1 37440/60160][Iteration 117][Wall Clock 26.353199351s] Trained 320.0 records in 0.215235327 seconds. Throughput is 1486.7449 records/second. Loss is 0.15185812. \n",
      "2021-02-18 16:42:03 INFO  DistriOptimizer$:427 - [Epoch 1 37760/60160][Iteration 118][Wall Clock 26.567673919s] Trained 320.0 records in 0.214474568 seconds. Throughput is 1492.0183 records/second. Loss is 0.05162517. \n",
      "2021-02-18 16:42:03 INFO  DistriOptimizer$:427 - [Epoch 1 38080/60160][Iteration 119][Wall Clock 26.784366677s] Trained 320.0 records in 0.216692758 seconds. Throughput is 1476.7452 records/second. Loss is 0.12618582. \n",
      "2021-02-18 16:42:03 INFO  DistriOptimizer$:427 - [Epoch 1 38400/60160][Iteration 120][Wall Clock 27.000979948s] Trained 320.0 records in 0.216613271 seconds. Throughput is 1477.2871 records/second. Loss is 0.14333457. \n",
      "2021-02-18 16:42:03 INFO  DistriOptimizer$:427 - [Epoch 1 38720/60160][Iteration 121][Wall Clock 27.214315349s] Trained 320.0 records in 0.213335401 seconds. Throughput is 1499.9855 records/second. Loss is 0.12189905. \n",
      "2021-02-18 16:42:04 INFO  DistriOptimizer$:427 - [Epoch 1 39040/60160][Iteration 122][Wall Clock 27.429270264s] Trained 320.0 records in 0.214954915 seconds. Throughput is 1488.6843 records/second. Loss is 0.109596945. \n",
      "2021-02-18 16:42:04 INFO  DistriOptimizer$:427 - [Epoch 1 39360/60160][Iteration 123][Wall Clock 27.642267008s] Trained 320.0 records in 0.212996744 seconds. Throughput is 1502.3704 records/second. Loss is 0.06208747. \n",
      "2021-02-18 16:42:04 INFO  DistriOptimizer$:427 - [Epoch 1 39680/60160][Iteration 124][Wall Clock 27.859364016s] Trained 320.0 records in 0.217097008 seconds. Throughput is 1473.9954 records/second. Loss is 0.11496117. \n",
      "2021-02-18 16:42:04 INFO  DistriOptimizer$:427 - [Epoch 1 40000/60160][Iteration 125][Wall Clock 28.073437879s] Trained 320.0 records in 0.214073863 seconds. Throughput is 1494.8112 records/second. Loss is 0.06972204. \n",
      "2021-02-18 16:42:04 INFO  DistriOptimizer$:427 - [Epoch 1 40320/60160][Iteration 126][Wall Clock 28.300471468s] Trained 320.0 records in 0.227033589 seconds. Throughput is 1409.483 records/second. Loss is 0.094318576. \n",
      "2021-02-18 16:42:05 INFO  DistriOptimizer$:427 - [Epoch 1 40640/60160][Iteration 127][Wall Clock 28.522196534s] Trained 320.0 records in 0.221725066 seconds. Throughput is 1443.2289 records/second. Loss is 0.079997905. \n",
      "2021-02-18 16:42:05 INFO  DistriOptimizer$:427 - [Epoch 1 40960/60160][Iteration 128][Wall Clock 28.741286271s] Trained 320.0 records in 0.219089737 seconds. Throughput is 1460.5887 records/second. Loss is 0.117596105. \n",
      "2021-02-18 16:42:05 INFO  DistriOptimizer$:427 - [Epoch 1 41280/60160][Iteration 129][Wall Clock 28.960995885s] Trained 320.0 records in 0.219709614 seconds. Throughput is 1456.4679 records/second. Loss is 0.05976491. \n",
      "2021-02-18 16:42:05 INFO  DistriOptimizer$:427 - [Epoch 1 41600/60160][Iteration 130][Wall Clock 29.177842492s] Trained 320.0 records in 0.216846607 seconds. Throughput is 1475.6975 records/second. Loss is 0.08211562. \n",
      "2021-02-18 16:42:06 INFO  DistriOptimizer$:427 - [Epoch 1 41920/60160][Iteration 131][Wall Clock 29.399833763s] Trained 320.0 records in 0.221991271 seconds. Throughput is 1441.4982 records/second. Loss is 0.10240854. \n",
      "2021-02-18 16:42:06 INFO  DistriOptimizer$:427 - [Epoch 1 42240/60160][Iteration 132][Wall Clock 29.614205557s] Trained 320.0 records in 0.214371794 seconds. Throughput is 1492.7336 records/second. Loss is 0.07771425. \n",
      "2021-02-18 16:42:06 INFO  DistriOptimizer$:427 - [Epoch 1 42560/60160][Iteration 133][Wall Clock 29.836151405s] Trained 320.0 records in 0.221945848 seconds. Throughput is 1441.7931 records/second. Loss is 0.081533715. \n",
      "2021-02-18 16:42:06 INFO  DistriOptimizer$:427 - [Epoch 1 42880/60160][Iteration 134][Wall Clock 30.04585398s] Trained 320.0 records in 0.209702575 seconds. Throughput is 1525.9707 records/second. Loss is 0.09862488. \n",
      "2021-02-18 16:42:06 INFO  DistriOptimizer$:427 - [Epoch 1 43200/60160][Iteration 135][Wall Clock 30.259329766s] Trained 320.0 records in 0.213475786 seconds. Throughput is 1498.9991 records/second. Loss is 0.073416136. \n",
      "2021-02-18 16:42:07 INFO  DistriOptimizer$:427 - [Epoch 1 43520/60160][Iteration 136][Wall Clock 30.472186451s] Trained 320.0 records in 0.212856685 seconds. Throughput is 1503.359 records/second. Loss is 0.12768963. \n",
      "2021-02-18 16:42:07 INFO  DistriOptimizer$:427 - [Epoch 1 43840/60160][Iteration 137][Wall Clock 30.698204008s] Trained 320.0 records in 0.226017557 seconds. Throughput is 1415.8192 records/second. Loss is 0.086020276. \n",
      "2021-02-18 16:42:07 INFO  DistriOptimizer$:427 - [Epoch 1 44160/60160][Iteration 138][Wall Clock 30.9103282s] Trained 320.0 records in 0.212124192 seconds. Throughput is 1508.5502 records/second. Loss is 0.096209146. \n",
      "2021-02-18 16:42:07 INFO  DistriOptimizer$:427 - [Epoch 1 44480/60160][Iteration 139][Wall Clock 31.122102031s] Trained 320.0 records in 0.211773831 seconds. Throughput is 1511.046 records/second. Loss is 0.09889687. \n",
      "2021-02-18 16:42:07 INFO  DistriOptimizer$:427 - [Epoch 1 44800/60160][Iteration 140][Wall Clock 31.340699723s] Trained 320.0 records in 0.218597692 seconds. Throughput is 1463.8763 records/second. Loss is 0.09994894. \n",
      "2021-02-18 16:42:08 INFO  DistriOptimizer$:427 - [Epoch 1 45120/60160][Iteration 141][Wall Clock 31.551602104s] Trained 320.0 records in 0.210902381 seconds. Throughput is 1517.2897 records/second. Loss is 0.09115498. \n",
      "2021-02-18 16:42:08 INFO  DistriOptimizer$:427 - [Epoch 1 45440/60160][Iteration 142][Wall Clock 31.766903247s] Trained 320.0 records in 0.215301143 seconds. Throughput is 1486.2903 records/second. Loss is 0.06621194. \n",
      "2021-02-18 16:42:08 INFO  DistriOptimizer$:427 - [Epoch 1 45760/60160][Iteration 143][Wall Clock 31.978283816s] Trained 320.0 records in 0.211380569 seconds. Throughput is 1513.8572 records/second. Loss is 0.093683705. \n",
      "2021-02-18 16:42:08 INFO  DistriOptimizer$:427 - [Epoch 1 46080/60160][Iteration 144][Wall Clock 32.193978598s] Trained 320.0 records in 0.215694782 seconds. Throughput is 1483.5779 records/second. Loss is 0.08219786. \n",
      "2021-02-18 16:42:09 INFO  DistriOptimizer$:427 - [Epoch 1 46400/60160][Iteration 145][Wall Clock 32.408799358s] Trained 320.0 records in 0.21482076 seconds. Throughput is 1489.614 records/second. Loss is 0.06885565. \n",
      "2021-02-18 16:42:09 INFO  DistriOptimizer$:427 - [Epoch 1 46720/60160][Iteration 146][Wall Clock 32.616907734s] Trained 320.0 records in 0.208108376 seconds. Throughput is 1537.6603 records/second. Loss is 0.0602728. \n",
      "2021-02-18 16:42:09 INFO  DistriOptimizer$:427 - [Epoch 1 47040/60160][Iteration 147][Wall Clock 32.833329757s] Trained 320.0 records in 0.216422023 seconds. Throughput is 1478.5927 records/second. Loss is 0.109630585. \n",
      "2021-02-18 16:42:09 INFO  DistriOptimizer$:427 - [Epoch 1 47360/60160][Iteration 148][Wall Clock 33.04806362s] Trained 320.0 records in 0.214733863 seconds. Throughput is 1490.2167 records/second. Loss is 0.064837605. \n",
      "2021-02-18 16:42:09 INFO  DistriOptimizer$:427 - [Epoch 1 47680/60160][Iteration 149][Wall Clock 33.269415247s] Trained 320.0 records in 0.221351627 seconds. Throughput is 1445.6637 records/second. Loss is 0.1110996. \n",
      "2021-02-18 16:42:10 INFO  DistriOptimizer$:427 - [Epoch 1 48000/60160][Iteration 150][Wall Clock 33.485501938s] Trained 320.0 records in 0.216086691 seconds. Throughput is 1480.8872 records/second. Loss is 0.076689444. \n",
      "2021-02-18 16:42:10 INFO  DistriOptimizer$:427 - [Epoch 1 48320/60160][Iteration 151][Wall Clock 33.698643513s] Trained 320.0 records in 0.213141575 seconds. Throughput is 1501.3495 records/second. Loss is 0.049552463. \n",
      "2021-02-18 16:42:10 INFO  DistriOptimizer$:427 - [Epoch 1 48640/60160][Iteration 152][Wall Clock 33.912609701s] Trained 320.0 records in 0.213966188 seconds. Throughput is 1495.5634 records/second. Loss is 0.08723225. \n",
      "2021-02-18 16:42:10 INFO  DistriOptimizer$:427 - [Epoch 1 48960/60160][Iteration 153][Wall Clock 34.121827285s] Trained 320.0 records in 0.209217584 seconds. Throughput is 1529.5082 records/second. Loss is 0.05237745. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-18 16:42:11 INFO  DistriOptimizer$:427 - [Epoch 1 49280/60160][Iteration 154][Wall Clock 34.336829952s] Trained 320.0 records in 0.215002667 seconds. Throughput is 1488.3536 records/second. Loss is 0.06714714. \n",
      "2021-02-18 16:42:11 INFO  DistriOptimizer$:427 - [Epoch 1 49600/60160][Iteration 155][Wall Clock 34.549025707s] Trained 320.0 records in 0.212195755 seconds. Throughput is 1508.0415 records/second. Loss is 0.07002549. \n",
      "2021-02-18 16:42:11 INFO  DistriOptimizer$:427 - [Epoch 1 49920/60160][Iteration 156][Wall Clock 34.770275408s] Trained 320.0 records in 0.221249701 seconds. Throughput is 1446.3296 records/second. Loss is 0.18483515. \n",
      "2021-02-18 16:42:11 INFO  DistriOptimizer$:427 - [Epoch 1 50240/60160][Iteration 157][Wall Clock 34.984838116s] Trained 320.0 records in 0.214562708 seconds. Throughput is 1491.4054 records/second. Loss is 0.0683266. \n",
      "2021-02-18 16:42:11 INFO  DistriOptimizer$:427 - [Epoch 1 50560/60160][Iteration 158][Wall Clock 35.195254038s] Trained 320.0 records in 0.210415922 seconds. Throughput is 1520.7975 records/second. Loss is 0.080975704. \n",
      "2021-02-18 16:42:12 INFO  DistriOptimizer$:427 - [Epoch 1 50880/60160][Iteration 159][Wall Clock 35.410593191s] Trained 320.0 records in 0.215339153 seconds. Throughput is 1486.028 records/second. Loss is 0.06988149. \n",
      "2021-02-18 16:42:12 INFO  DistriOptimizer$:427 - [Epoch 1 51200/60160][Iteration 160][Wall Clock 35.627484162s] Trained 320.0 records in 0.216890971 seconds. Throughput is 1475.3956 records/second. Loss is 0.06374325. \n",
      "2021-02-18 16:42:12 INFO  DistriOptimizer$:427 - [Epoch 1 51520/60160][Iteration 161][Wall Clock 35.840513672s] Trained 320.0 records in 0.21302951 seconds. Throughput is 1502.1394 records/second. Loss is 0.06423535. \n",
      "2021-02-18 16:42:12 INFO  DistriOptimizer$:427 - [Epoch 1 51840/60160][Iteration 162][Wall Clock 36.065389271s] Trained 320.0 records in 0.224875599 seconds. Throughput is 1423.009 records/second. Loss is 0.035232775. \n",
      "2021-02-18 16:42:12 INFO  DistriOptimizer$:427 - [Epoch 1 52160/60160][Iteration 163][Wall Clock 36.282661802s] Trained 320.0 records in 0.217272531 seconds. Throughput is 1472.8047 records/second. Loss is 0.12875353. \n",
      "2021-02-18 16:42:13 INFO  DistriOptimizer$:427 - [Epoch 1 52480/60160][Iteration 164][Wall Clock 36.496916616s] Trained 320.0 records in 0.214254814 seconds. Throughput is 1493.5487 records/second. Loss is 0.06669669. \n",
      "2021-02-18 16:42:13 INFO  DistriOptimizer$:427 - [Epoch 1 52800/60160][Iteration 165][Wall Clock 36.708418429s] Trained 320.0 records in 0.211501813 seconds. Throughput is 1512.9894 records/second. Loss is 0.12783153. \n",
      "2021-02-18 16:42:13 INFO  DistriOptimizer$:427 - [Epoch 1 53120/60160][Iteration 166][Wall Clock 36.930265058s] Trained 320.0 records in 0.221846629 seconds. Throughput is 1442.438 records/second. Loss is 0.11573331. \n",
      "2021-02-18 16:42:13 INFO  DistriOptimizer$:427 - [Epoch 1 53440/60160][Iteration 167][Wall Clock 37.143891009s] Trained 320.0 records in 0.213625951 seconds. Throughput is 1497.9453 records/second. Loss is 0.09370735. \n",
      "2021-02-18 16:42:14 INFO  DistriOptimizer$:427 - [Epoch 1 53760/60160][Iteration 168][Wall Clock 37.358436615s] Trained 320.0 records in 0.214545606 seconds. Throughput is 1491.5244 records/second. Loss is 0.10571201. \n",
      "2021-02-18 16:42:14 INFO  DistriOptimizer$:427 - [Epoch 1 54080/60160][Iteration 169][Wall Clock 37.576580465s] Trained 320.0 records in 0.21814385 seconds. Throughput is 1466.922 records/second. Loss is 0.09031318. \n",
      "2021-02-18 16:42:14 INFO  DistriOptimizer$:427 - [Epoch 1 54400/60160][Iteration 170][Wall Clock 37.800318601s] Trained 320.0 records in 0.223738136 seconds. Throughput is 1430.2434 records/second. Loss is 0.052260734. \n",
      "2021-02-18 16:42:14 INFO  DistriOptimizer$:427 - [Epoch 1 54720/60160][Iteration 171][Wall Clock 38.019889227s] Trained 320.0 records in 0.219570626 seconds. Throughput is 1457.3899 records/second. Loss is 0.09444794. \n",
      "2021-02-18 16:42:14 INFO  DistriOptimizer$:427 - [Epoch 1 55040/60160][Iteration 172][Wall Clock 38.238246783s] Trained 320.0 records in 0.218357556 seconds. Throughput is 1465.4862 records/second. Loss is 0.069328465. \n",
      "2021-02-18 16:42:15 INFO  DistriOptimizer$:427 - [Epoch 1 55360/60160][Iteration 173][Wall Clock 38.453112731s] Trained 320.0 records in 0.214865948 seconds. Throughput is 1489.3007 records/second. Loss is 0.06836179. \n",
      "2021-02-18 16:42:15 INFO  DistriOptimizer$:427 - [Epoch 1 55680/60160][Iteration 174][Wall Clock 38.665337389s] Trained 320.0 records in 0.212224658 seconds. Throughput is 1507.836 records/second. Loss is 0.15319183. \n",
      "2021-02-18 16:42:15 INFO  DistriOptimizer$:427 - [Epoch 1 56000/60160][Iteration 175][Wall Clock 38.876412823s] Trained 320.0 records in 0.211075434 seconds. Throughput is 1516.0457 records/second. Loss is 0.1036977. \n",
      "2021-02-18 16:42:15 INFO  DistriOptimizer$:427 - [Epoch 1 56320/60160][Iteration 176][Wall Clock 39.090433997s] Trained 320.0 records in 0.214021174 seconds. Throughput is 1495.1792 records/second. Loss is 0.08326951. \n",
      "2021-02-18 16:42:15 INFO  DistriOptimizer$:427 - [Epoch 1 56640/60160][Iteration 177][Wall Clock 39.304216614s] Trained 320.0 records in 0.213782617 seconds. Throughput is 1496.8475 records/second. Loss is 0.06595429. \n",
      "2021-02-18 16:42:16 INFO  DistriOptimizer$:427 - [Epoch 1 56960/60160][Iteration 178][Wall Clock 39.512097985s] Trained 320.0 records in 0.207881371 seconds. Throughput is 1539.3395 records/second. Loss is 0.07263695. \n",
      "2021-02-18 16:42:16 INFO  DistriOptimizer$:427 - [Epoch 1 57280/60160][Iteration 179][Wall Clock 39.728126484s] Trained 320.0 records in 0.216028499 seconds. Throughput is 1481.286 records/second. Loss is 0.12219272. \n",
      "2021-02-18 16:42:16 INFO  DistriOptimizer$:427 - [Epoch 1 57600/60160][Iteration 180][Wall Clock 39.943321733s] Trained 320.0 records in 0.215195249 seconds. Throughput is 1487.0216 records/second. Loss is 0.07729708. \n",
      "2021-02-18 16:42:16 INFO  DistriOptimizer$:427 - [Epoch 1 57920/60160][Iteration 181][Wall Clock 40.156229118s] Trained 320.0 records in 0.212907385 seconds. Throughput is 1503.0009 records/second. Loss is 0.05669611. \n",
      "2021-02-18 16:42:17 INFO  DistriOptimizer$:427 - [Epoch 1 58240/60160][Iteration 182][Wall Clock 40.371637861s] Trained 320.0 records in 0.215408743 seconds. Throughput is 1485.5479 records/second. Loss is 0.056713678. \n",
      "2021-02-18 16:42:17 INFO  DistriOptimizer$:427 - [Epoch 1 58560/60160][Iteration 183][Wall Clock 40.587325083s] Trained 320.0 records in 0.215687222 seconds. Throughput is 1483.6299 records/second. Loss is 0.09142937. \n",
      "2021-02-18 16:42:17 INFO  DistriOptimizer$:427 - [Epoch 1 58880/60160][Iteration 184][Wall Clock 40.801236343s] Trained 320.0 records in 0.21391126 seconds. Throughput is 1495.9474 records/second. Loss is 0.114922844. \n",
      "2021-02-18 16:42:17 INFO  DistriOptimizer$:427 - [Epoch 1 59200/60160][Iteration 185][Wall Clock 41.01345461s] Trained 320.0 records in 0.212218267 seconds. Throughput is 1507.8815 records/second. Loss is 0.054783784. \n",
      "2021-02-18 16:42:17 INFO  DistriOptimizer$:427 - [Epoch 1 59520/60160][Iteration 186][Wall Clock 41.230457989s] Trained 320.0 records in 0.217003379 seconds. Throughput is 1474.6315 records/second. Loss is 0.06414381. \n",
      "2021-02-18 16:42:18 INFO  DistriOptimizer$:427 - [Epoch 1 59840/60160][Iteration 187][Wall Clock 41.442966731s] Trained 320.0 records in 0.212508742 seconds. Throughput is 1505.8204 records/second. Loss is 0.06614671. \n",
      "2021-02-18 16:42:18 INFO  DistriOptimizer$:427 - [Epoch 1 60160/60160][Iteration 188][Wall Clock 41.564655077s] Trained 320.0 records in 0.121688346 seconds. Throughput is 2629.6685 records/second. Loss is 0.11077849. \n",
      "2021-02-18 16:42:18 INFO  DistriOptimizer$:472 - [Epoch 1 60160/60160][Iteration 188][Wall Clock 41.564655077s] Epoch finished. Wall clock time is 41802.885934 ms\n",
      "2021-02-18 16:42:18 INFO  DistriOptimizer$:111 - [Epoch 1 60160/60160][Iteration 188][Wall Clock 41.564655077s] Validate model...\n",
      "2021-02-18 16:42:18 ERROR ThreadPool$:136 - Error: java.lang.IllegalArgumentException: requirement failed: tensor size not match 320x10 320\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor$.expandSize(DenseTensor.scala:2790)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor.expandTensor(DenseTensor.scala:979)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor.add(DenseTensor.scala:973)\n",
      "\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:36)\n",
      "\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)\n",
      "\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:32)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:27)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:158)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:157)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:157)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:141)\n",
      "\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invokeAndWait$1$$anonfun$apply$3.apply(ThreadPool.scala:133)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-02-18 16:42:18 ERROR Executor:91 - Exception in task 0.0 in stage 385.0 (TID 385)\n",
      "java.lang.IllegalArgumentException: requirement failed: tensor size not match 320x10 320\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor$.expandSize(DenseTensor.scala:2790)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor.expandTensor(DenseTensor.scala:979)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor.add(DenseTensor.scala:973)\n",
      "\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:36)\n",
      "\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)\n",
      "\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:32)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:27)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:158)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:157)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:157)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:141)\n",
      "\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invokeAndWait$1$$anonfun$apply$3.apply(ThreadPool.scala:133)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2021-02-18 16:42:18 ERROR TaskSetManager:70 - Task 0 in stage 385.0 failed 1 times; aborting job\n",
      "2021-02-18 16:42:18 ERROR DistriOptimizer$:1287 - Error: java.lang.reflect.InvocationTargetException\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.layers.utils.KerasUtils$.invokeMethod(KerasUtils.scala:302)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.layers.utils.KerasUtils$.invokeMethodWithEv(KerasUtils.scala:329)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalOptimizerUtil$.optimizeModels(Topology.scala:1063)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalDistriOptimizer.train(Topology.scala:1262)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalDistriOptimizer.train(Topology.scala:1475)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalDistriOptimizer.train(Topology.scala:1145)\n",
      "\tat com.intel.analytics.zoo.pipeline.estimator.Estimator.train(Estimator.scala:190)\n",
      "\tat com.intel.analytics.zoo.pipeline.estimator.python.PythonEstimator.estimatorTrainMiniBatch(PythonEstimator.scala:117)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 385.0 failed 1 times, most recent failure: Lost task 0.0 in stage 385.0 (TID 385, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: tensor size not match 320x10 320\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor$.expandSize(DenseTensor.scala:2790)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor.expandTensor(DenseTensor.scala:979)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor.add(DenseTensor.scala:973)\n",
      "\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:36)\n",
      "\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)\n",
      "\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:32)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:27)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:158)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:157)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:157)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:141)\n",
      "\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invokeAndWait$1$$anonfun$apply$3.apply(ThreadPool.scala:133)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer.validate(AbstractOptimizer.scala:168)\n",
      "\tat com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:490)\n",
      "\t... 23 more\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: tensor size not match 320x10 320\n",
      "\tat scala.Predef$.require(Predef.scala:224)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor$.expandSize(DenseTensor.scala:2790)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor.expandTensor(DenseTensor.scala:979)\n",
      "\tat com.intel.analytics.bigdl.tensor.DenseTensor.add(DenseTensor.scala:973)\n",
      "\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:36)\n",
      "\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)\n",
      "\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:32)\n",
      "\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:27)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:158)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:157)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:157)\n",
      "\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:141)\n",
      "\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invokeAndWait$1$$anonfun$apply$3.apply(ThreadPool.scala:133)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o51.estimatorTrainMiniBatch.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 385.0 failed 1 times, most recent failure: Lost task 0.0 in stage 385.0 (TID 385, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: tensor size not match 320x10 320\n\tat scala.Predef$.require(Predef.scala:224)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor$.expandSize(DenseTensor.scala:2790)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor.expandTensor(DenseTensor.scala:979)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor.add(DenseTensor.scala:973)\n\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:36)\n\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)\n\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)\n\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:32)\n\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:27)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:158)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:157)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:157)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:141)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invokeAndWait$1$$anonfun$apply$3.apply(ThreadPool.scala:133)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer.validate(AbstractOptimizer.scala:168)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:490)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.intel.analytics.zoo.pipeline.api.keras.layers.utils.KerasUtils$.invokeMethod(KerasUtils.scala:302)\n\tat com.intel.analytics.zoo.pipeline.api.keras.layers.utils.KerasUtils$.invokeMethodWithEv(KerasUtils.scala:329)\n\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalOptimizerUtil$.optimizeModels(Topology.scala:1063)\n\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalDistriOptimizer.train(Topology.scala:1262)\n\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalDistriOptimizer.train(Topology.scala:1475)\n\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalDistriOptimizer.train(Topology.scala:1145)\n\tat com.intel.analytics.zoo.pipeline.estimator.Estimator.train(Estimator.scala:190)\n\tat com.intel.analytics.zoo.pipeline.estimator.python.PythonEstimator.estimatorTrainMiniBatch(PythonEstimator.scala:117)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: tensor size not match 320x10 320\n\tat scala.Predef$.require(Predef.scala:224)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor$.expandSize(DenseTensor.scala:2790)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor.expandTensor(DenseTensor.scala:979)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor.add(DenseTensor.scala:973)\n\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:36)\n\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)\n\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)\n\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:32)\n\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:27)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:158)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:157)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:157)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:141)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invokeAndWait$1$$anonfun$apply$3.apply(ThreadPool.scala:133)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7acee70aba8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m est.fit(data=train_loader, epochs=1, validation_data=test_loader,\n\u001b[0;32m----> 4\u001b[0;31m         checkpoint_trigger=EveryEpoch())\n\u001b[0m",
      "\u001b[0;32m~/analytics-zoo/pyzoo/zoo/orca/learn/pytorch/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, epochs, batch_size, feature_cols, label_cols, validation_data, checkpoint_trigger)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mtrain_fset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_fset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hanle_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             self.estimator.train_minibatch(train_fset, self.loss, end_trigger,\n\u001b[0;32m--> 298\u001b[0;31m                                            checkpoint_trigger, val_fset, self.metrics)\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             raise ValueError(\"Data and validation data should be SparkXShards, DataLoaders or \"\n",
      "\u001b[0;32m~/analytics-zoo/pyzoo/zoo/pipeline/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, train_set, criterion, end_trigger, checkpoint_trigger, validation_set, validation_method)\u001b[0m\n\u001b[1;32m    166\u001b[0m         callZooFunc(self.bigdl_type, \"estimatorTrainMiniBatch\", self.value, train_set,\n\u001b[1;32m    167\u001b[0m                     \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_trigger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_trigger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                     validation_method)\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analytics-zoo/pyzoo/zoo/common/utils.py\u001b[0m in \u001b[0;36mcallZooFunc\u001b[0;34m(bigdl_type, name, *args)\u001b[0m\n\u001b[1;32m    131\u001b[0m             if not (\"does not exist\" in str(e)\n\u001b[1;32m    132\u001b[0m                     and \"Method {}\".format(name) in str(e)):\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/analytics-zoo/pyzoo/zoo/common/utils.py\u001b[0m in \u001b[0;36mcallZooFunc\u001b[0;34m(bigdl_type, name, *args)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjinvoker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mjava_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o51.estimatorTrainMiniBatch.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 385.0 failed 1 times, most recent failure: Lost task 0.0 in stage 385.0 (TID 385, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: tensor size not match 320x10 320\n\tat scala.Predef$.require(Predef.scala:224)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor$.expandSize(DenseTensor.scala:2790)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor.expandTensor(DenseTensor.scala:979)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor.add(DenseTensor.scala:973)\n\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:36)\n\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)\n\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)\n\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:32)\n\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:27)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:158)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:157)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:157)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:141)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invokeAndWait$1$$anonfun$apply$3.apply(ThreadPool.scala:133)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer.validate(AbstractOptimizer.scala:168)\n\tat com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:490)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.intel.analytics.zoo.pipeline.api.keras.layers.utils.KerasUtils$.invokeMethod(KerasUtils.scala:302)\n\tat com.intel.analytics.zoo.pipeline.api.keras.layers.utils.KerasUtils$.invokeMethodWithEv(KerasUtils.scala:329)\n\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalOptimizerUtil$.optimizeModels(Topology.scala:1063)\n\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalDistriOptimizer.train(Topology.scala:1262)\n\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalDistriOptimizer.train(Topology.scala:1475)\n\tat com.intel.analytics.zoo.pipeline.api.keras.models.InternalDistriOptimizer.train(Topology.scala:1145)\n\tat com.intel.analytics.zoo.pipeline.estimator.Estimator.train(Estimator.scala:190)\n\tat com.intel.analytics.zoo.pipeline.estimator.python.PythonEstimator.estimatorTrainMiniBatch(PythonEstimator.scala:117)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: tensor size not match 320x10 320\n\tat scala.Predef$.require(Predef.scala:224)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor$.expandSize(DenseTensor.scala:2790)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor.expandTensor(DenseTensor.scala:979)\n\tat com.intel.analytics.bigdl.tensor.DenseTensor.add(DenseTensor.scala:973)\n\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:36)\n\tat com.intel.analytics.bigdl.nn.AbsCriterion.updateOutput(AbsCriterion.scala:28)\n\tat com.intel.analytics.bigdl.nn.abstractnn.AbstractCriterion.forward(AbstractCriterion.scala:73)\n\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:32)\n\tat com.intel.analytics.zoo.pipeline.api.keras.metrics.MAE.apply(MAE.scala:27)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:158)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6$$anonfun$apply$9.apply(AbstractOptimizer.scala:157)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:157)\n\tat com.intel.analytics.bigdl.optim.AbstractOptimizer$$anonfun$2$$anonfun$apply$5$$anonfun$3$$anonfun$apply$6.apply(AbstractOptimizer.scala:141)\n\tat com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$invokeAndWait$1$$anonfun$apply$3.apply(ThreadPool.scala:133)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from zoo.orca.learn.trigger import EveryEpoch \n",
    "\n",
    "est.fit(data=train_loader, epochs=1, validation_data=test_loader,\n",
    "        checkpoint_trigger=EveryEpoch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = est.evaluate(data=test_loader)\n",
    "for r in result:\n",
    "    print(str(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-advertising",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
